{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingwen0/systemeAQ/blob/main/AQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7c554db0",
      "metadata": {
        "id": "7c554db0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# ### 第一部分：对于训练数据的处理：读取文件和预处理\n",
        "\n",
        "# - ```文本的读取```： 需要从文本中读取数据，此处需要读取的文件是```dev-v2.0.json```，并把读取的文件存入一个列表里（list）\n",
        "# - ```文本预处理```： 对于问题本身需要做一些停用词过滤等文本方面的处理\n",
        "# - ```可视化分析```： 对于给定的样本数据，做一些可视化分析来更好地理解数据\n",
        "\n",
        "\n",
        "# #### 1.1节： 文本的读取\n",
        "# 把给定的文本数据读入到```qlist```和```alist```当中，这两个分别是列表，其中```qlist```是问题的列表，```alist```是对应的答案列表"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget -P data https://raw.githubusercontent.com/jingwen0/systemeAQ/main/piaf-v1.2.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu5pSBZyKU3_",
        "outputId": "fdd0181e-f53e-4559-cef6-4fa56787db9a"
      },
      "id": "pu5pSBZyKU3_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dat’: File exists\n",
            "--2022-06-04 19:53:46--  https://raw.githubusercontent.com/jingwen0/systemeAQ/main/piaf-v1.2.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4744747 (4.5M) [text/plain]\n",
            "Saving to: ‘data/piaf-v1.2.json.1’\n",
            "\n",
            "piaf-v1.2.json.1    100%[===================>]   4.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-06-04 19:53:46 (228 MB/s) - ‘data/piaf-v1.2.json.1’ saved [4744747/4744747]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fd9b4de7",
      "metadata": {
        "id": "fd9b4de7"
      },
      "outputs": [],
      "source": [
        "# #### 1.1节： 文本的读取\n",
        "# 把给定的文本数据读入到```qlist```和```alist```当中，这两个分别是列表，其中```qlist```是问题的列表，```alist```是对应的答案列表\n",
        "def read_corpus():\n",
        "    \"\"\"\n",
        "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
        "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
        "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
        "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
        "    \"\"\"\n",
        "    # 问题列表\n",
        "    qlist = []\n",
        "    # 答案列表\n",
        "    alist = []\n",
        "    # 加载json文件\n",
        "    # 加载json文件\n",
        "    filename = 'data/piaf-v1.2.json'\n",
        "    datas = json.load(open(filename, 'r'))\n",
        "    data = datas['data']\n",
        "    print(datas)\n",
        "    for d in data:\n",
        "        paragraph = d['paragraphs']\n",
        "        for p in paragraph:\n",
        "            qas = p['qas']\n",
        "            for qa in qas:\n",
        "                # 处理is_impossible为True时answers空\n",
        "                # if (not qa['is_impossible']):\n",
        "                qlist.append(qa['question'])\n",
        "                alist.append(qa['answers'][0]['text'])\n",
        "                  \n",
        "    # 如果它的条件返回错误，则终止程序执行\n",
        "    # 这行代码是确保长度一样\n",
        "    assert len(qlist) == len(alist)\n",
        "    return qlist, alist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "56d04bf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d04bf6",
        "outputId": "c3c29974-506d-4265-8682-c6733fda85d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qlist的单词统计： 13783\n"
          ]
        }
      ],
      "source": [
        "# 读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist\n",
        "qlist, alist = read_corpus()\n",
        "\n",
        "# ### 1.2 理解数据（可视化分析/统计信息）\n",
        "# 统计一下在qlist中总共出现了多少个单词？ 总共出现了多少个不同的单词(unique word)？\n",
        "# 这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
        "words_qlist = dict()\n",
        "for q in qlist:\n",
        "    # 以空格为分词，都转为小写\n",
        "    words = q.strip().split(' ')\n",
        "    for w in words:\n",
        "        if w.lower() in words_qlist:\n",
        "            words_qlist[w.lower()] += 1\n",
        "        else:\n",
        "            words_qlist[w.lower()] = 1\n",
        "word_total = len(words_qlist)\n",
        "print(\"qlist的单词统计：\",word_total)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "8e44748a",
      "metadata": {
        "id": "8e44748a"
      },
      "source": [
        "# 统计一下qlist中出现1次，2次，3次... 出现的单词个数， 然后画一个plot. 这里的x轴是单词出现的次数（1，2，3，..)， y轴是单词个数。\n",
        "# 从左到右分别是 出现1次的单词数，出现2次的单词数，出现3次的单词数\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# counts：key出现N次，value：出现N次词有多少\n",
        "counts = dict()\n",
        "for w, c in words_qlist.items():\n",
        "    if c in counts:\n",
        "        counts[c] += 1\n",
        "    else:\n",
        "        counts[c] = 1\n",
        "# 以histogram画图\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(counts.values(), bins=np.arange(0, 250, 25), histtype='step', alpha=0.6, label=\"counts\")\n",
        "ax.legend()\n",
        "ax.set_xlim(0, 250)\n",
        "ax.set_yticks(np.arange(0, 500, 50))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b7e8a8a6",
      "metadata": {
        "id": "b7e8a8a6"
      },
      "outputs": [],
      "source": [
        "# #### 1.3 文本预处理\n",
        "# 此部分需要做文本方面的处理。 以下是可以用到的一些方法：\n",
        "#\n",
        "# - 1. 停用词过滤 （去网上搜一下 \"english stop words list\"，会出现很多包含停用词库的网页，或者直接使用NLTK自带的）\n",
        "# - 2. 转换成lower_case： 这是一个基本的操作\n",
        "# - 3. 去掉一些无用的符号： 比如连续的感叹号！！！， 或者一些奇怪的单词。\n",
        "# - 4. 去掉出现频率很低的词：比如出现次数少于10,20.... （想一下如何选择阈值）\n",
        "# - 5. 对于数字的处理： 分词完只有有些单词可能就是数字比如44，415，把所有这些数字都看成是一个单词，这个新的单词我们可以定义为 \"#number\"\n",
        "# - 6. lemmazation： 在这里不要使用stemming， 因为stemming的结果有可能不是valid word。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "06912d58",
      "metadata": {
        "id": "06912d58"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "\n",
        "# 去掉一些无用的符号\n",
        "def tokenizer(ori_list):\n",
        "    # 利用正则表达式去掉无用的符号\n",
        "    # compile 函数用于编译正则表达式，[]用来表示一组字符\n",
        "    # \\s匹配任意空白字符，等价于 [\\t\\n\\r\\f]。\n",
        "    SYMBOLS = re.compile('[\\s;\\\"\\\",.!?\\\\/\\[\\]\\{\\}\\(\\)-]+')\n",
        "    new_list = []\n",
        "    for q in ori_list:\n",
        "        # split 方法按照能够匹配的子串将字符串分割后返回列表\n",
        "        words = SYMBOLS.split(q.lower().strip())\n",
        "        new_list.append(' '.join(words))\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5b736cef",
      "metadata": {
        "id": "5b736cef"
      },
      "outputs": [],
      "source": [
        "# 去掉question的停用词\n",
        "def removeStopWord(ori_list):\n",
        "    new_list = []\n",
        "    # nltk中stopwords包含what等，但是在QA问题中，这算关键词，所以不看作关键词\n",
        "    restored = ['que', 'qui', 'quoi', 'où', 'comment', 'pourquoi','combien','quel','quels','quelle','quelles']\n",
        "    # nltk中自带的停用词库\n",
        "    french_stop_words = list(\n",
        "        set(stopwords.words('french')))  \n",
        "    # 将在QA问答系统中不算停用词的词去掉\n",
        "    for w in restored:\n",
        "      if w in french_stop_words:\n",
        "          french_stop_words.remove(w)\n",
        "      else:\n",
        "          pass\n",
        "    for q in ori_list:\n",
        "        # 将每个问句的停用词去掉\n",
        "        sentence = ' '.join([w for w in q.strip().split(' ') if w not in french_stop_words])\n",
        "        # 将去掉停用词的问句添加至列表中\n",
        "        new_list.append(sentence)\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f8a45a5a",
      "metadata": {
        "id": "f8a45a5a"
      },
      "outputs": [],
      "source": [
        "def removeLowFrequence(ori_list, vocabulary, thres=10):\n",
        "    \"\"\"\n",
        "    去掉低频率的词\n",
        "    :param ori_list: 预处理后的问题列表\n",
        "    :param vocabulary: 词频率字典\n",
        "    :param thres: 频率阈值，可以基于数据实际情况进行调整\n",
        "    :return: 新的问题列表\n",
        "    \"\"\"\n",
        "    # 根据thres筛选词表，小于thres的词去掉\n",
        "    new_list = []\n",
        "    for q in ori_list:\n",
        "        sentence = ' '.join([w for w in q.strip().split(' ') if vocabulary[w] >= thres])\n",
        "        new_list.append(sentence)\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "83b70a5f",
      "metadata": {
        "id": "83b70a5f"
      },
      "outputs": [],
      "source": [
        "def replaceDigits(ori_list, replace='#number'):\n",
        "    \"\"\"\n",
        "    将数字统一替换为replace,默认#number\n",
        "    :param ori_list: 预处理后的问题列表\n",
        "    :param replace:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 编译正则表达式：匹配1个或多个数字\n",
        "    DIGITS = re.compile('\\d+')\n",
        "    new_list = []\n",
        "    for q in ori_list:\n",
        "        # re.sub用于替换字符串中的匹配项，相当于在q中查找连续的数字替换为#number\n",
        "        q = DIGITS.sub(replace, q)\n",
        "        # 将处理后的问题字符串添加到新列表中\n",
        "        new_list.append(q)\n",
        "    return new_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "57b1ec08",
      "metadata": {
        "id": "57b1ec08"
      },
      "outputs": [],
      "source": [
        "def createVocab(ori_list):\n",
        "    \"\"\"\n",
        "    创建词表，统计所有单词总数与每个单词总数\n",
        "    :param ori_list:预处理后的列表\n",
        "    :return:所有单词总数与每个单词总数\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    vocab_count = dict()\n",
        "    for q in ori_list:\n",
        "        words = q.strip().split(' ')\n",
        "        count += len(words)\n",
        "        for w in words:\n",
        "            if w in vocab_count:\n",
        "                vocab_count[w] += 1\n",
        "            else:\n",
        "                vocab_count[w] = 1\n",
        "    return vocab_count, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "76aaad15",
      "metadata": {
        "id": "76aaad15"
      },
      "outputs": [],
      "source": [
        "def writeFile(oriList, filename):\n",
        "    \"\"\"\n",
        "    将处理后的问题列表写入到文件中\n",
        "    :param oriList: 预处理后的问题列表\n",
        "    :param filename: 文件名\n",
        "    \"\"\"\n",
        "    with codecs.open(filename, 'w', 'utf8') as Fout:\n",
        "        for q in oriList:\n",
        "            Fout.write(q + u'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f1b17bf7",
      "metadata": {
        "id": "f1b17bf7"
      },
      "outputs": [],
      "source": [
        "def writeVocab(vocabulary, filename):\n",
        "    \"\"\"\n",
        "    将词表写入到文件中\n",
        "    :param vocabulary: 词表\n",
        "    :param filename: 文件名\n",
        "    \"\"\"\n",
        "    sortedList = sorted(vocabulary.items(), key=lambda d: d[1])\n",
        "    with codecs.open(filename, 'w', 'utf8') as Fout:\n",
        "        for (w, c) in sortedList:\n",
        "            Fout.write(w + u':' + str(c) + u'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0e826473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e826473",
        "outputId": "5442a292-0044-4ecc-9b5a-e5f70aeae6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['combien personnes ministère', 'combien', 'quel part budget', 'quel montant #number', 'quel montant #number', 'qui a', 'que trouve sous mont', 'qui hors commun', 'a lieu quel moment', 'qui a fait', 'qui constitution', 'qui grand père', 'parents personnes', 'qui apprendre', \"comment s'appelait a #number ans\", 'quelle année tombe', 'qui mort', 'quel métier nicolas', 'que doit avant derniers', 'pourquoi', 'quand a thèse', 'a quel âge obtient', 'quelles études suit guillaume', 'a quel âge guillaume marie andré part paris', 'combien temps a duré', 'comment nil nuit', 'comment nil', 'quel mois froid nil', 'comment nil', \"comment l'air nil\", \"quel pays surnommé l'afrique\", 'quelle croissance moyenne années #number', 'qui vice parlement #number #number', 'comment', 'quelle proportion femmes parlement #number', 'quand guerre', 'quel pays passe accord #number', 'quelle institution fait', 'qui #number', 'quel groupe #number', 'qui dirigeait #number', 'qui', 'qui a fait thèse', 'quel pays a étudié', 'où', \"quelle l'origine nom\", 'quel pays', 'qui père', 'quelle nationalité', 'où', 'quel débute', 'quel court ulis', \"quelle activité propose l'association ulis\", 'combien début quartier', 'bord quel route trouve quartier', 'a combien paris trouvent ulis', 'quel point toutes routes france', 'a combien situés ulis', 'ulis sud', 'ulis trois', \"quoi l'école paris pouvait être certains années #number\", 'quand lieu babylone organisée charles', \"quand l'expression paris a\", 'quels nouvelle paris', 'quel courant artistique babylone janvier #number', 'quelle forme', 'quelle figure caractérise', 'année décès', 'quelle naissance', 'citez figure musulman', 'quelle organisation internationale cameroun france après défaite allemande', \"quelle nouvelle forme politique coloniale introduit france cameroun rapport l'allemagne\", 'quelle puissance coloniale présente', 'quelle année france cameroun', \"pourquoi l'allemagne\", 'quelle scientifiques allemands durant période coloniale', 'populations elles conditions sous allemande', 'quelle trouve depuis période coloniale cameroun', 'quels travaux manière cameroun', 'quelle nationalité décide cameroun', \"quelle profession l'armée cameroun pendant allemande\", 'qui a influencé allemand projet colonial cameroun', \"qui l'allemagne lorsqu'elle cameroun\", \"quel l'allemagne cameroun\", 'quelle date allemands cameroun', \"quel royaume a l'influence religions cameroun\", \"comment s'appelle royaume\", 'quels premiers cameroun', 'qui premiers cameroun', 'qui a territoire après portugal', 'quel pays associé tribus', 'quand remonte premières cameroun', 'qui premiers cameroun', \"qu'est qui a langues continent africain\", 'langues elles continent africain', '', 'comment', 'qui john', \"comment s'appelle famille\", 'pourquoi']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "# 去掉一些无用的符号\n",
        "new_list = tokenizer(qlist)\n",
        "# 停用词过滤\n",
        "new_list = removeStopWord(new_list)\n",
        "# 数字处理-将数字替换为#number\n",
        "new_list = replaceDigits(new_list)\n",
        "# 创建词表并统计所有单词数目\n",
        "vocabulary, count = createVocab(new_list)\n",
        "# 去掉低频率的词\n",
        "new_list = removeLowFrequence(new_list, vocabulary, 5)\n",
        "# 重新统计词频\n",
        "vocab_count, count = createVocab(new_list)\n",
        "# 将词表写入到文件“train.vocab”中\n",
        "writeVocab(vocab_count, \"train.vocab\")\n",
        "qlist = new_list\n",
        "print(qlist[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fcf773b",
      "metadata": {
        "id": "9fcf773b"
      },
      "outputs": [],
      "source": [
        "# ### 第二部分： 文本的表示\n",
        "# 文本处理完之后，我们需要做文本表示，这里有3种方式：\n",
        "#\n",
        "# - 1. 使用```tf-idf vector```\n",
        "# - 2. 使用embedding技术如```word2vec```, ```bert embedding```等\n",
        "\n",
        "# #### 2.1 使用tf-idf表示向量\n",
        "# 把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里。\n",
        "# # ``X``的大小是： ``N* D``的矩阵。 这里``N``是问题的个数（样本个数），``D``是词典库的大小\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d6a13b4c",
      "metadata": {
        "id": "d6a13b4c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def computeTF(vocab, c):\n",
        "    \"\"\"\n",
        "    计算每个词的词频\n",
        "    :param vocab: 词频字典:键是单词，值是所有问句中单词出现的次数\n",
        "    :param c: 单词总数\n",
        "    :return: TF\n",
        "    \"\"\"\n",
        "    # 初始化TF\n",
        "    TF = np.ones(len(vocab))\n",
        "    # 词频字典\n",
        "    word2id = dict()\n",
        "    # 单词字典\n",
        "    id2word = dict()\n",
        "    for word, fre in vocab.items():\n",
        "        # 计算TF值：每个单词出现的个数/总的单词个数\n",
        "        TF[len(word2id)] = 1.0 * fre / c\n",
        "        id2word[len(word2id)] = word\n",
        "        word2id[word] = len(word2id)\n",
        "    return TF, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1b97aa8c",
      "metadata": {
        "id": "1b97aa8c"
      },
      "outputs": [],
      "source": [
        "def computeIDF(word2id, qlist):\n",
        "    \"\"\"\n",
        "    计算IDF：log[问句总数/(包含单词t的问句总数+1)]\n",
        "    :param word2id:单词字典\n",
        "    :param qlist:问句列表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    IDF = np.ones(len(word2id))\n",
        "    for q in qlist:\n",
        "        # 去重\n",
        "        words = set(q.strip().split())\n",
        "        for w in words:\n",
        "            # 统计单词出现在问句中的总数\n",
        "            IDF[word2id[w]] += 1\n",
        "    # 计算IDF\n",
        "    IDF /= len(qlist)\n",
        "    IDF = -1.0 * np.log2(IDF)\n",
        "    return IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d3c405e4",
      "metadata": {
        "id": "d3c405e4"
      },
      "outputs": [],
      "source": [
        "def computeSentenceEach(sentence, tfidf, word2id):\n",
        "    \"\"\"\n",
        "    给定句子，计算句子TF-IDF,tfidf是一个1*M的矩阵,M为词表大小\n",
        "    :param sentence:句子\n",
        "    :param tfidf:TF-IDF向量\n",
        "    :param word2id:词表\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    sentence_tfidf = np.zeros(len(word2id))\n",
        "    # 将问句以空格进行分割\n",
        "    for w in sentence.strip().split(' '):\n",
        "        if w not in word2id:\n",
        "            continue\n",
        "        # 碰到在词表word2id中的单词\n",
        "        sentence_tfidf[word2id[w]] = tfidf[word2id[w]]\n",
        "    return sentence_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1f31f291",
      "metadata": {
        "id": "1f31f291"
      },
      "outputs": [],
      "source": [
        "def computeSentence(qlist, word2id, tfidf):\n",
        "    \"\"\"\n",
        "    把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里\n",
        "    :param qlist: 问题列表\n",
        "    :param word2id: 词表(字典形式)\n",
        "    :param tfidf: TF-IDF(与词表中的键一一对应)\n",
        "    :return: X矩阵\n",
        "    \"\"\"\n",
        "    # 对所有句子分别求tfidf\n",
        "    X_tfidf = np.zeros((len(qlist), len(word2id)))\n",
        "    for i, q in enumerate(qlist):\n",
        "        X_tfidf[i] = computeSentenceEach(q, tfidf, word2id)\n",
        "        # print(X_tfidf[i])\n",
        "    return X_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ae023d2a",
      "metadata": {
        "id": "ae023d2a"
      },
      "outputs": [],
      "source": [
        "# 计算每个词的TF,词表字典\n",
        "TF, word2id, id2word = computeTF(vocab_count, count)\n",
        "# 计算IDF：log[问句总数/(包含单词t的问句总数+1)]\n",
        "IDF = computeIDF(word2id, qlist)\n",
        "# 用TF，IDF计算最终的tf-idf,定义一个tf-idf的vectorizer\n",
        "vectorizer = np.multiply(TF, IDF)\n",
        "# 把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里\n",
        "X_tfidf = computeSentence(qlist, word2id, vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4dfbdcff",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dfbdcff",
        "outputId": "a78d3657-d96a-4b9c-b573-1a780a2e13ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149858 sha256=4522a82c1dfc5677ceeef21ccd7c076687ef8bcbd71fb9ee27a89aadf4efab0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086376e0",
      "metadata": {
        "id": "086376e0"
      },
      "outputs": [],
      "source": [
        "# #### 2.2 使用wordvec + average pooling\n",
        "# 词向量方面需要下载： https://nlp.stanford.edu/projects/glove/ （请下载``glove.6B.zip``），并使用``d=200``的词向量（200维）。\n",
        "# 国外网址如果很慢，可以在百度上搜索国内服务器上的。 每个词向量获取完之后，即可以得到一个句子的向量。 我们通过``average pooling``来实现句子的向量。\n",
        "# 基于Glove向量获取句子向量\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "model_path = '/content/data/glove.6B.200d.txt'"
      ],
      "metadata": {
        "id": "r8uGP1EgI4c2"
      },
      "id": "r8uGP1EgI4c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21572ffa",
      "metadata": {
        "id": "21572ffa"
      },
      "outputs": [],
      "source": [
        "def loadEmbedding(filename):\n",
        "    \"\"\"\n",
        "    加载glove模型，转化为word2vec，再加载到word2vec模型\n",
        "    这两种模型形式上是一样的，在数据的保存形式上有略微的差异\n",
        "    :param filename: glove文件\n",
        "    :return: word2vec模型\n",
        "    \"\"\"\n",
        "    word2vec_temp_file = get_tmpfile(\"test_word2vec.txt\")\n",
        "    # 加载glove模型，转化为word2vec\n",
        "    glove2word2vec(filename, word2vec_temp_file)\n",
        "    # 再加载到word2vec模型\n",
        "    model = KeyedVectors.load_word2vec_format(word2vec_temp_file)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P data https://git.unistra.fr/dbernhard/aisd_d3/-/raw/master/data/fra_newscrawl-public_2019_300K-tokenised-sentences.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkYqf5C3vMKc",
        "outputId": "23b8755e-672f-4e01-ad4b-9117343e648e"
      },
      "id": "TkYqf5C3vMKc",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-04 19:56:07--  https://git.unistra.fr/dbernhard/aisd_d3/-/raw/master/data/fra_newscrawl-public_2019_300K-tokenised-sentences.txt\n",
            "Resolving git.unistra.fr (git.unistra.fr)... 130.79.254.48\n",
            "Connecting to git.unistra.fr (git.unistra.fr)|130.79.254.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38143498 (36M) [text/plain]\n",
            "Saving to: ‘data/fra_newscrawl-public_2019_300K-tokenised-sentences.txt’\n",
            "\n",
            "fra_newscrawl-publi 100%[===================>]  36.38M  19.1MB/s    in 1.9s    \n",
            "\n",
            "2022-06-04 19:56:10 (19.1 MB/s) - ‘data/fra_newscrawl-public_2019_300K-tokenised-sentences.txt’ saved [38143498/38143498]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "import pandas as pd\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)\n",
        "# Pour mieux afficher les textes (colonnes plus larges)\n",
        "pd.set_option('display.max_colwidth', 600) "
      ],
      "metadata": {
        "id": "73pjIanzvalj"
      },
      "id": "73pjIanzvalj",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_corpus = LineSentence('data/fra_newscrawl-public_2019_300K-tokenised-sentences.txt')\n",
        "# Dimension des vecteurs\n",
        "VEC_SIZE = 300\n",
        "# Taille de la fenêtre de contexte\n",
        "WINDOW_SIZE = 5\n",
        "# Nombre minimal d'occurrences pour les mots\n",
        "MIN_FREQ = 1\n",
        "# Nombre d'itérations sur le corpus complet\n",
        "NB_EPOCHS = 10\n",
        "# Nombre de threads (fils d'exécution) pour accélérer l'apprentissage\n",
        "NB_WORKERS = 4\n",
        "# Nombre de mots « négatifs » à sélectionner pour l'échantillonnage négatif\n",
        "NEGATIVE_SAMPLE = 5\n",
        "# Seuil pour sélectionner les mots fréquents qui sont sous-échantillonnés\n",
        "SUB_SAMPLING = 0.00001\n",
        "# Taux d'apprentissage (learning rate) initial : contrôle l'amplitude des ajustements effectués sur les poids du réseau\n",
        "LR = 0.025\n",
        "# Valeur minimale pour le taux d'apprentissage\n",
        "MIN_LR = 0.0001\n",
        "# Utilisation de Skip-gram\n",
        "SG = 1"
      ],
      "metadata": {
        "id": "5j7ISnPOvRj2"
      },
      "id": "5j7ISnPOvRj2",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=input_corpus, \n",
        "                 size=VEC_SIZE, window=WINDOW_SIZE,\n",
        "                 min_count=MIN_FREQ, iter=NB_EPOCHS,\n",
        "                 sg=SG, workers=NB_WORKERS,\n",
        "                 negative=NEGATIVE_SAMPLE,\n",
        "                 sample=SUB_SAMPLING, alpha=LR,\n",
        "                 min_alpha=MIN_LR, compute_loss=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNHRduf9vgWm",
        "outputId": "365102ec-717d-4587-f5ca-410cf52ff60b"
      },
      "id": "GNHRduf9vgWm",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-06-04 20:24:03,005 : INFO : collecting all words and their counts\n",
            "2022-06-04 20:24:03,008 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-06-04 20:24:03,145 : INFO : PROGRESS: at sentence #10000, processed 252527 words, keeping 30145 word types\n",
            "2022-06-04 20:24:03,260 : INFO : PROGRESS: at sentence #20000, processed 503173 words, keeping 44394 word types\n",
            "2022-06-04 20:24:03,374 : INFO : PROGRESS: at sentence #30000, processed 728801 words, keeping 55092 word types\n",
            "2022-06-04 20:24:03,474 : INFO : PROGRESS: at sentence #40000, processed 956669 words, keeping 62463 word types\n",
            "2022-06-04 20:24:03,579 : INFO : PROGRESS: at sentence #50000, processed 1172099 words, keeping 68513 word types\n",
            "2022-06-04 20:24:03,690 : INFO : PROGRESS: at sentence #60000, processed 1406090 words, keeping 76743 word types\n",
            "2022-06-04 20:24:03,803 : INFO : PROGRESS: at sentence #70000, processed 1662992 words, keeping 84206 word types\n",
            "2022-06-04 20:24:03,922 : INFO : PROGRESS: at sentence #80000, processed 1889455 words, keeping 90861 word types\n",
            "2022-06-04 20:24:04,030 : INFO : PROGRESS: at sentence #90000, processed 2126434 words, keeping 96784 word types\n",
            "2022-06-04 20:24:04,136 : INFO : PROGRESS: at sentence #100000, processed 2358956 words, keeping 102041 word types\n",
            "2022-06-04 20:24:04,238 : INFO : PROGRESS: at sentence #110000, processed 2581924 words, keeping 108240 word types\n",
            "2022-06-04 20:24:04,340 : INFO : PROGRESS: at sentence #120000, processed 2799409 words, keeping 111806 word types\n",
            "2022-06-04 20:24:04,449 : INFO : PROGRESS: at sentence #130000, processed 3008332 words, keeping 116146 word types\n",
            "2022-06-04 20:24:04,547 : INFO : PROGRESS: at sentence #140000, processed 3221840 words, keeping 120565 word types\n",
            "2022-06-04 20:24:04,651 : INFO : PROGRESS: at sentence #150000, processed 3454502 words, keeping 124947 word types\n",
            "2022-06-04 20:24:04,756 : INFO : PROGRESS: at sentence #160000, processed 3685856 words, keeping 129383 word types\n",
            "2022-06-04 20:24:04,867 : INFO : PROGRESS: at sentence #170000, processed 3926506 words, keeping 134255 word types\n",
            "2022-06-04 20:24:04,977 : INFO : PROGRESS: at sentence #180000, processed 4171473 words, keeping 138492 word types\n",
            "2022-06-04 20:24:05,088 : INFO : PROGRESS: at sentence #190000, processed 4399378 words, keeping 142397 word types\n",
            "2022-06-04 20:24:05,198 : INFO : PROGRESS: at sentence #200000, processed 4623123 words, keeping 146135 word types\n",
            "2022-06-04 20:24:05,306 : INFO : PROGRESS: at sentence #210000, processed 4858098 words, keeping 150185 word types\n",
            "2022-06-04 20:24:05,417 : INFO : PROGRESS: at sentence #220000, processed 5076338 words, keeping 153719 word types\n",
            "2022-06-04 20:24:05,519 : INFO : PROGRESS: at sentence #230000, processed 5298435 words, keeping 157888 word types\n",
            "2022-06-04 20:24:05,620 : INFO : PROGRESS: at sentence #240000, processed 5512012 words, keeping 161028 word types\n",
            "2022-06-04 20:24:05,728 : INFO : PROGRESS: at sentence #250000, processed 5748853 words, keeping 165364 word types\n",
            "2022-06-04 20:24:05,835 : INFO : PROGRESS: at sentence #260000, processed 5982097 words, keeping 169268 word types\n",
            "2022-06-04 20:24:05,944 : INFO : PROGRESS: at sentence #270000, processed 6214843 words, keeping 173856 word types\n",
            "2022-06-04 20:24:06,067 : INFO : PROGRESS: at sentence #280000, processed 6454743 words, keeping 178037 word types\n",
            "2022-06-04 20:24:06,188 : INFO : PROGRESS: at sentence #290000, processed 6668452 words, keeping 180941 word types\n",
            "2022-06-04 20:24:06,288 : INFO : collected 184393 word types from a corpus of 6873735 raw words and 300000 sentences\n",
            "2022-06-04 20:24:06,289 : INFO : Loading a fresh vocabulary\n",
            "2022-06-04 20:24:06,869 : INFO : effective_min_count=1 retains 184393 unique words (100% of original 184393, drops 0)\n",
            "2022-06-04 20:24:06,876 : INFO : effective_min_count=1 leaves 6873735 word corpus (100% of original 6873735, drops 0)\n",
            "2022-06-04 20:24:07,425 : INFO : deleting the raw counts dictionary of 184393 items\n",
            "2022-06-04 20:24:07,431 : INFO : sample=1e-05 downsamples 3270 most-common words\n",
            "2022-06-04 20:24:07,437 : INFO : downsampling leaves estimated 2227593 word corpus (32.4% of prior 6873735)\n",
            "2022-06-04 20:24:08,010 : INFO : estimated required memory for 184393 words and 300 dimensions: 534739700 bytes\n",
            "2022-06-04 20:24:08,013 : INFO : resetting layer weights\n",
            "2022-06-04 20:24:42,708 : INFO : training model with 4 workers on 184393 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=5\n",
            "2022-06-04 20:24:43,731 : INFO : EPOCH 1 - PROGRESS: at 4.03% examples, 100164 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:44,732 : INFO : EPOCH 1 - PROGRESS: at 8.47% examples, 103204 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:24:45,738 : INFO : EPOCH 1 - PROGRESS: at 13.81% examples, 105367 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:46,776 : INFO : EPOCH 1 - PROGRESS: at 19.18% examples, 105945 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:47,800 : INFO : EPOCH 1 - PROGRESS: at 23.54% examples, 105877 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:48,868 : INFO : EPOCH 1 - PROGRESS: at 28.87% examples, 106713 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:49,874 : INFO : EPOCH 1 - PROGRESS: at 34.06% examples, 107889 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:50,883 : INFO : EPOCH 1 - PROGRESS: at 39.69% examples, 108716 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:51,961 : INFO : EPOCH 1 - PROGRESS: at 45.87% examples, 109177 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:52,997 : INFO : EPOCH 1 - PROGRESS: at 51.04% examples, 109644 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:54,041 : INFO : EPOCH 1 - PROGRESS: at 55.94% examples, 109726 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:55,043 : INFO : EPOCH 1 - PROGRESS: at 60.46% examples, 109505 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:56,058 : INFO : EPOCH 1 - PROGRESS: at 65.48% examples, 109660 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:57,059 : INFO : EPOCH 1 - PROGRESS: at 70.38% examples, 109892 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:24:58,068 : INFO : EPOCH 1 - PROGRESS: at 75.35% examples, 109645 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:24:59,150 : INFO : EPOCH 1 - PROGRESS: at 80.84% examples, 109178 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:25:00,169 : INFO : EPOCH 1 - PROGRESS: at 83.64% examples, 106555 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:25:01,189 : INFO : EPOCH 1 - PROGRESS: at 86.07% examples, 103654 words/s, in_qsize 8, out_qsize 2\n",
            "2022-06-04 20:25:02,199 : INFO : EPOCH 1 - PROGRESS: at 90.99% examples, 104307 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:03,218 : INFO : EPOCH 1 - PROGRESS: at 96.09% examples, 104686 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:03,809 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:25:03,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:25:03,837 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:25:03,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:25:03,857 : INFO : EPOCH - 1 : training on 6873735 raw words (2227640 effective words) took 21.1s, 105368 effective words/s\n",
            "2022-06-04 20:25:04,919 : INFO : EPOCH 2 - PROGRESS: at 4.32% examples, 102571 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:05,922 : INFO : EPOCH 2 - PROGRESS: at 9.12% examples, 107039 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:06,966 : INFO : EPOCH 2 - PROGRESS: at 14.73% examples, 108106 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:08,001 : INFO : EPOCH 2 - PROGRESS: at 20.03% examples, 108949 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:09,010 : INFO : EPOCH 2 - PROGRESS: at 24.57% examples, 109053 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:25:10,070 : INFO : EPOCH 2 - PROGRESS: at 29.87% examples, 109777 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:11,076 : INFO : EPOCH 2 - PROGRESS: at 34.97% examples, 110106 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:12,136 : INFO : EPOCH 2 - PROGRESS: at 40.91% examples, 110270 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:13,215 : INFO : EPOCH 2 - PROGRESS: at 47.00% examples, 110814 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:14,240 : INFO : EPOCH 2 - PROGRESS: at 52.05% examples, 110833 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:15,272 : INFO : EPOCH 2 - PROGRESS: at 56.78% examples, 110682 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:16,281 : INFO : EPOCH 2 - PROGRESS: at 61.65% examples, 110908 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:17,318 : INFO : EPOCH 2 - PROGRESS: at 66.67% examples, 110795 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:18,318 : INFO : EPOCH 2 - PROGRESS: at 71.79% examples, 110868 words/s, in_qsize 7, out_qsize 2\n",
            "2022-06-04 20:25:19,334 : INFO : EPOCH 2 - PROGRESS: at 76.26% examples, 110038 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:20,402 : INFO : EPOCH 2 - PROGRESS: at 81.88% examples, 110101 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:21,476 : INFO : EPOCH 2 - PROGRESS: at 86.98% examples, 109974 words/s, in_qsize 8, out_qsize 2\n",
            "2022-06-04 20:25:22,516 : INFO : EPOCH 2 - PROGRESS: at 91.83% examples, 110182 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:23,505 : INFO : EPOCH 2 - PROGRESS: at 97.18% examples, 110463 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:23,930 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:25:23,958 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:25:23,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:25:23,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:25:23,975 : INFO : EPOCH - 2 : training on 6873735 raw words (2228498 effective words) took 20.1s, 110803 effective words/s\n",
            "2022-06-04 20:25:25,024 : INFO : EPOCH 3 - PROGRESS: at 4.04% examples, 97791 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:26,035 : INFO : EPOCH 3 - PROGRESS: at 9.12% examples, 107536 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:27,039 : INFO : EPOCH 3 - PROGRESS: at 14.74% examples, 109740 words/s, in_qsize 5, out_qsize 2\n",
            "2022-06-04 20:25:28,097 : INFO : EPOCH 3 - PROGRESS: at 20.02% examples, 109661 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:29,100 : INFO : EPOCH 3 - PROGRESS: at 24.56% examples, 109679 words/s, in_qsize 8, out_qsize 1\n",
            "2022-06-04 20:25:30,162 : INFO : EPOCH 3 - PROGRESS: at 29.76% examples, 109808 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:31,203 : INFO : EPOCH 3 - PROGRESS: at 34.96% examples, 110028 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:32,245 : INFO : EPOCH 3 - PROGRESS: at 40.75% examples, 110343 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:25:33,238 : INFO : EPOCH 3 - PROGRESS: at 46.31% examples, 110190 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:34,305 : INFO : EPOCH 3 - PROGRESS: at 51.61% examples, 110519 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:35,332 : INFO : EPOCH 3 - PROGRESS: at 56.22% examples, 110127 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:36,340 : INFO : EPOCH 3 - PROGRESS: at 61.04% examples, 110361 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:37,350 : INFO : EPOCH 3 - PROGRESS: at 66.37% examples, 111019 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:38,356 : INFO : EPOCH 3 - PROGRESS: at 71.63% examples, 111261 words/s, in_qsize 6, out_qsize 0\n",
            "2022-06-04 20:25:39,372 : INFO : EPOCH 3 - PROGRESS: at 76.43% examples, 110858 words/s, in_qsize 6, out_qsize 2\n",
            "2022-06-04 20:25:40,380 : INFO : EPOCH 3 - PROGRESS: at 82.16% examples, 111343 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:41,389 : INFO : EPOCH 3 - PROGRESS: at 86.83% examples, 110991 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:42,423 : INFO : EPOCH 3 - PROGRESS: at 91.40% examples, 110743 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:43,432 : INFO : EPOCH 3 - PROGRESS: at 96.71% examples, 111031 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:43,971 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:25:43,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:25:43,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:25:43,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:25:43,993 : INFO : EPOCH - 3 : training on 6873735 raw words (2227893 effective words) took 20.0s, 111347 effective words/s\n",
            "2022-06-04 20:25:45,066 : INFO : EPOCH 4 - PROGRESS: at 4.30% examples, 101464 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:25:46,079 : INFO : EPOCH 4 - PROGRESS: at 9.27% examples, 107531 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:47,112 : INFO : EPOCH 4 - PROGRESS: at 15.19% examples, 110443 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:25:48,117 : INFO : EPOCH 4 - PROGRESS: at 20.29% examples, 110999 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:25:49,131 : INFO : EPOCH 4 - PROGRESS: at 24.86% examples, 110659 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:50,136 : INFO : EPOCH 4 - PROGRESS: at 29.74% examples, 110500 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:51,151 : INFO : EPOCH 4 - PROGRESS: at 34.82% examples, 110645 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:52,182 : INFO : EPOCH 4 - PROGRESS: at 40.43% examples, 110642 words/s, in_qsize 7, out_qsize 2\n",
            "2022-06-04 20:25:53,187 : INFO : EPOCH 4 - PROGRESS: at 46.44% examples, 111387 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:54,209 : INFO : EPOCH 4 - PROGRESS: at 51.32% examples, 111078 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:55,229 : INFO : EPOCH 4 - PROGRESS: at 55.80% examples, 110402 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:56,230 : INFO : EPOCH 4 - PROGRESS: at 60.33% examples, 110115 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:25:57,262 : INFO : EPOCH 4 - PROGRESS: at 65.48% examples, 110431 words/s, in_qsize 6, out_qsize 2\n",
            "2022-06-04 20:25:58,256 : INFO : EPOCH 4 - PROGRESS: at 70.69% examples, 110945 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:25:59,261 : INFO : EPOCH 4 - PROGRESS: at 75.51% examples, 110503 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:00,278 : INFO : EPOCH 4 - PROGRESS: at 81.13% examples, 110656 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:01,316 : INFO : EPOCH 4 - PROGRESS: at 86.03% examples, 110614 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:26:02,382 : INFO : EPOCH 4 - PROGRESS: at 90.49% examples, 109903 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:03,433 : INFO : EPOCH 4 - PROGRESS: at 95.62% examples, 109948 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:04,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:26:04,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:26:04,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:26:04,203 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:26:04,205 : INFO : EPOCH - 4 : training on 6873735 raw words (2227805 effective words) took 20.2s, 110265 effective words/s\n",
            "2022-06-04 20:26:05,223 : INFO : EPOCH 5 - PROGRESS: at 4.04% examples, 100612 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:06,230 : INFO : EPOCH 5 - PROGRESS: at 9.12% examples, 109380 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:07,253 : INFO : EPOCH 5 - PROGRESS: at 14.89% examples, 111190 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:08,291 : INFO : EPOCH 5 - PROGRESS: at 20.29% examples, 112022 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:09,291 : INFO : EPOCH 5 - PROGRESS: at 24.86% examples, 111801 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:10,334 : INFO : EPOCH 5 - PROGRESS: at 30.01% examples, 111845 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:11,374 : INFO : EPOCH 5 - PROGRESS: at 35.27% examples, 111942 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:12,387 : INFO : EPOCH 5 - PROGRESS: at 41.58% examples, 113169 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:13,403 : INFO : EPOCH 5 - PROGRESS: at 47.29% examples, 113455 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:14,476 : INFO : EPOCH 5 - PROGRESS: at 52.33% examples, 112734 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:15,489 : INFO : EPOCH 5 - PROGRESS: at 57.20% examples, 112855 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:16,515 : INFO : EPOCH 5 - PROGRESS: at 62.09% examples, 112902 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:26:17,500 : INFO : EPOCH 5 - PROGRESS: at 67.12% examples, 112925 words/s, in_qsize 7, out_qsize 2\n",
            "2022-06-04 20:26:18,521 : INFO : EPOCH 5 - PROGRESS: at 72.53% examples, 113057 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:19,558 : INFO : EPOCH 5 - PROGRESS: at 77.85% examples, 112932 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:26:20,574 : INFO : EPOCH 5 - PROGRESS: at 83.25% examples, 113169 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:21,578 : INFO : EPOCH 5 - PROGRESS: at 88.06% examples, 112905 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:22,583 : INFO : EPOCH 5 - PROGRESS: at 92.55% examples, 112666 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:23,619 : INFO : EPOCH 5 - PROGRESS: at 98.28% examples, 112938 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:23,802 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:26:23,835 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:26:23,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:26:23,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:26:23,866 : INFO : EPOCH - 5 : training on 6873735 raw words (2227994 effective words) took 19.7s, 113359 effective words/s\n",
            "2022-06-04 20:26:24,886 : INFO : EPOCH 6 - PROGRESS: at 4.04% examples, 99810 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:25,900 : INFO : EPOCH 6 - PROGRESS: at 8.97% examples, 107115 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:26,919 : INFO : EPOCH 6 - PROGRESS: at 14.57% examples, 109035 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:27,923 : INFO : EPOCH 6 - PROGRESS: at 19.45% examples, 107725 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:26:28,943 : INFO : EPOCH 6 - PROGRESS: at 24.13% examples, 108532 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:26:30,010 : INFO : EPOCH 6 - PROGRESS: at 29.37% examples, 108855 words/s, in_qsize 8, out_qsize 2\n",
            "2022-06-04 20:26:31,025 : INFO : EPOCH 6 - PROGRESS: at 34.51% examples, 109380 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:32,090 : INFO : EPOCH 6 - PROGRESS: at 40.41% examples, 109848 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:33,112 : INFO : EPOCH 6 - PROGRESS: at 46.31% examples, 110268 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:26:34,212 : INFO : EPOCH 6 - PROGRESS: at 51.61% examples, 110232 words/s, in_qsize 5, out_qsize 2\n",
            "2022-06-04 20:26:35,225 : INFO : EPOCH 6 - PROGRESS: at 56.36% examples, 110311 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:36,226 : INFO : EPOCH 6 - PROGRESS: at 61.04% examples, 110295 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:37,300 : INFO : EPOCH 6 - PROGRESS: at 65.78% examples, 109428 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:38,323 : INFO : EPOCH 6 - PROGRESS: at 70.84% examples, 109632 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:39,361 : INFO : EPOCH 6 - PROGRESS: at 76.42% examples, 110027 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:40,419 : INFO : EPOCH 6 - PROGRESS: at 81.88% examples, 109874 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:41,431 : INFO : EPOCH 6 - PROGRESS: at 86.98% examples, 110175 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:42,443 : INFO : EPOCH 6 - PROGRESS: at 90.99% examples, 109382 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:43,489 : INFO : EPOCH 6 - PROGRESS: at 95.94% examples, 109183 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:44,126 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:26:44,151 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:26:44,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:26:44,161 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:26:44,161 : INFO : EPOCH - 6 : training on 6873735 raw words (2226183 effective words) took 20.3s, 109720 effective words/s\n",
            "2022-06-04 20:26:45,171 : INFO : EPOCH 7 - PROGRESS: at 3.90% examples, 98314 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:26:46,224 : INFO : EPOCH 7 - PROGRESS: at 8.81% examples, 104378 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:47,269 : INFO : EPOCH 7 - PROGRESS: at 14.57% examples, 107256 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:48,298 : INFO : EPOCH 7 - PROGRESS: at 20.03% examples, 109103 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:49,301 : INFO : EPOCH 7 - PROGRESS: at 24.56% examples, 109242 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:50,310 : INFO : EPOCH 7 - PROGRESS: at 29.37% examples, 108873 words/s, in_qsize 8, out_qsize 2\n",
            "2022-06-04 20:26:51,328 : INFO : EPOCH 7 - PROGRESS: at 34.51% examples, 109327 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:52,343 : INFO : EPOCH 7 - PROGRESS: at 39.97% examples, 109340 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:53,353 : INFO : EPOCH 7 - PROGRESS: at 45.87% examples, 109918 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:54,360 : INFO : EPOCH 7 - PROGRESS: at 50.74% examples, 109984 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:55,366 : INFO : EPOCH 7 - PROGRESS: at 55.54% examples, 110108 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:56,408 : INFO : EPOCH 7 - PROGRESS: at 60.18% examples, 109769 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:26:57,417 : INFO : EPOCH 7 - PROGRESS: at 65.18% examples, 109962 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:58,445 : INFO : EPOCH 7 - PROGRESS: at 70.22% examples, 110225 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:26:59,453 : INFO : EPOCH 7 - PROGRESS: at 75.21% examples, 109925 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:00,524 : INFO : EPOCH 7 - PROGRESS: at 80.97% examples, 110008 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:01,612 : INFO : EPOCH 7 - PROGRESS: at 86.01% examples, 109845 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:27:02,618 : INFO : EPOCH 7 - PROGRESS: at 90.84% examples, 110041 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:03,668 : INFO : EPOCH 7 - PROGRESS: at 95.92% examples, 109945 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:04,296 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:27:04,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:27:04,316 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:27:04,348 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:27:04,351 : INFO : EPOCH - 7 : training on 6873735 raw words (2228374 effective words) took 20.2s, 110416 effective words/s\n",
            "2022-06-04 20:27:05,369 : INFO : EPOCH 8 - PROGRESS: at 3.90% examples, 97225 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:06,370 : INFO : EPOCH 8 - PROGRESS: at 8.83% examples, 106314 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:07,415 : INFO : EPOCH 8 - PROGRESS: at 13.95% examples, 104920 words/s, in_qsize 7, out_qsize 3\n",
            "2022-06-04 20:27:08,486 : INFO : EPOCH 8 - PROGRESS: at 19.87% examples, 108198 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:09,491 : INFO : EPOCH 8 - PROGRESS: at 24.28% examples, 107928 words/s, in_qsize 6, out_qsize 2\n",
            "2022-06-04 20:27:10,516 : INFO : EPOCH 8 - PROGRESS: at 29.38% examples, 108571 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:11,517 : INFO : EPOCH 8 - PROGRESS: at 34.36% examples, 108854 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:12,554 : INFO : EPOCH 8 - PROGRESS: at 40.10% examples, 109440 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:27:13,609 : INFO : EPOCH 8 - PROGRESS: at 46.30% examples, 110238 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:14,627 : INFO : EPOCH 8 - PROGRESS: at 51.33% examples, 110406 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:15,633 : INFO : EPOCH 8 - PROGRESS: at 55.68% examples, 109664 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:16,680 : INFO : EPOCH 8 - PROGRESS: at 60.18% examples, 109006 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:17,753 : INFO : EPOCH 8 - PROGRESS: at 62.95% examples, 104987 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:18,785 : INFO : EPOCH 8 - PROGRESS: at 65.78% examples, 101880 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:27:19,825 : INFO : EPOCH 8 - PROGRESS: at 70.53% examples, 102070 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:27:20,827 : INFO : EPOCH 8 - PROGRESS: at 75.80% examples, 102775 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:21,845 : INFO : EPOCH 8 - PROGRESS: at 81.41% examples, 103390 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:22,870 : INFO : EPOCH 8 - PROGRESS: at 86.15% examples, 103680 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:27:23,923 : INFO : EPOCH 8 - PROGRESS: at 90.97% examples, 103839 words/s, in_qsize 7, out_qsize 1\n",
            "2022-06-04 20:27:24,933 : INFO : EPOCH 8 - PROGRESS: at 95.94% examples, 104137 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:25,518 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:27:25,563 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:27:25,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:27:25,589 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:27:25,592 : INFO : EPOCH - 8 : training on 6873735 raw words (2226933 effective words) took 21.2s, 104880 effective words/s\n",
            "2022-06-04 20:27:26,625 : INFO : EPOCH 9 - PROGRESS: at 3.65% examples, 89404 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:27,631 : INFO : EPOCH 9 - PROGRESS: at 8.13% examples, 98797 words/s, in_qsize 4, out_qsize 3\n",
            "2022-06-04 20:27:28,638 : INFO : EPOCH 9 - PROGRESS: at 13.82% examples, 104732 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:29,642 : INFO : EPOCH 9 - PROGRESS: at 19.06% examples, 105473 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:30,659 : INFO : EPOCH 9 - PROGRESS: at 23.13% examples, 104333 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:27:31,703 : INFO : EPOCH 9 - PROGRESS: at 28.21% examples, 104908 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:32,725 : INFO : EPOCH 9 - PROGRESS: at 33.14% examples, 105678 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:33,756 : INFO : EPOCH 9 - PROGRESS: at 38.74% examples, 106786 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:34,837 : INFO : EPOCH 9 - PROGRESS: at 44.95% examples, 107098 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:35,844 : INFO : EPOCH 9 - PROGRESS: at 49.86% examples, 107558 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:36,895 : INFO : EPOCH 9 - PROGRESS: at 54.71% examples, 107416 words/s, in_qsize 5, out_qsize 2\n",
            "2022-06-04 20:27:37,915 : INFO : EPOCH 9 - PROGRESS: at 59.39% examples, 107553 words/s, in_qsize 6, out_qsize 2\n",
            "2022-06-04 20:27:38,950 : INFO : EPOCH 9 - PROGRESS: at 64.46% examples, 107983 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:39,984 : INFO : EPOCH 9 - PROGRESS: at 69.29% examples, 107735 words/s, in_qsize 8, out_qsize 2\n",
            "2022-06-04 20:27:41,003 : INFO : EPOCH 9 - PROGRESS: at 74.61% examples, 108144 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:42,008 : INFO : EPOCH 9 - PROGRESS: at 80.01% examples, 108205 words/s, in_qsize 7, out_qsize 2\n",
            "2022-06-04 20:27:43,081 : INFO : EPOCH 9 - PROGRESS: at 85.03% examples, 108260 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:27:44,084 : INFO : EPOCH 9 - PROGRESS: at 89.96% examples, 108619 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:45,103 : INFO : EPOCH 9 - PROGRESS: at 94.53% examples, 108368 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:45,953 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:27:45,970 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:27:46,003 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:27:46,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:27:46,015 : INFO : EPOCH - 9 : training on 6873735 raw words (2228440 effective words) took 20.4s, 109144 effective words/s\n",
            "2022-06-04 20:27:47,061 : INFO : EPOCH 10 - PROGRESS: at 4.18% examples, 100952 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:48,062 : INFO : EPOCH 10 - PROGRESS: at 8.97% examples, 106514 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:49,088 : INFO : EPOCH 10 - PROGRESS: at 14.09% examples, 105646 words/s, in_qsize 8, out_qsize 2\n",
            "2022-06-04 20:27:50,107 : INFO : EPOCH 10 - PROGRESS: at 19.47% examples, 106965 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:51,125 : INFO : EPOCH 10 - PROGRESS: at 23.99% examples, 107274 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:52,133 : INFO : EPOCH 10 - PROGRESS: at 29.13% examples, 108433 words/s, in_qsize 8, out_qsize 0\n",
            "2022-06-04 20:27:53,160 : INFO : EPOCH 10 - PROGRESS: at 34.21% examples, 108716 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:54,213 : INFO : EPOCH 10 - PROGRESS: at 39.82% examples, 108805 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:55,223 : INFO : EPOCH 10 - PROGRESS: at 45.87% examples, 109731 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:56,234 : INFO : EPOCH 10 - PROGRESS: at 50.74% examples, 109752 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:27:57,244 : INFO : EPOCH 10 - PROGRESS: at 55.54% examples, 109853 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:58,286 : INFO : EPOCH 10 - PROGRESS: at 60.32% examples, 109779 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:27:59,351 : INFO : EPOCH 10 - PROGRESS: at 65.48% examples, 109783 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:28:00,358 : INFO : EPOCH 10 - PROGRESS: at 70.38% examples, 109858 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:28:01,396 : INFO : EPOCH 10 - PROGRESS: at 75.80% examples, 110092 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:28:02,409 : INFO : EPOCH 10 - PROGRESS: at 81.13% examples, 109949 words/s, in_qsize 6, out_qsize 1\n",
            "2022-06-04 20:28:03,459 : INFO : EPOCH 10 - PROGRESS: at 86.20% examples, 110028 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:28:04,464 : INFO : EPOCH 10 - PROGRESS: at 90.99% examples, 110206 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:28:05,466 : INFO : EPOCH 10 - PROGRESS: at 96.08% examples, 110383 words/s, in_qsize 7, out_qsize 0\n",
            "2022-06-04 20:28:06,078 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2022-06-04 20:28:06,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-06-04 20:28:06,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-06-04 20:28:06,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-06-04 20:28:06,125 : INFO : EPOCH - 10 : training on 6873735 raw words (2227696 effective words) took 20.1s, 110820 effective words/s\n",
            "2022-06-04 20:28:06,126 : INFO : training on a 68737350 raw words (22277456 effective words) took 203.4s, 109517 effective words/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "c78b938b",
      "metadata": {
        "id": "c78b938b"
      },
      "outputs": [],
      "source": [
        "def computeGloveSentenceEach(sentence, embedding):\n",
        "    \"\"\"\n",
        "    :param sentence:问题\n",
        "    :param embedding:wordvec模型\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 查找句子中每个词的embedding,将所有embedding进行加和求均值\n",
        "    emb = np.zeros(300)\n",
        "    words = sentence.strip().split(' ')\n",
        "    for w in words:\n",
        "      # 如果单词是新词，则重命名为'unknown'\n",
        "      if w not in embedding:\n",
        "            # 没有lookup的即为unknown\n",
        "          w = 'unknown'\n",
        "      else:\n",
        "        pass\n",
        "        # 将所有embedding进行加和求均值\n",
        "        emb += embedding.wv.get_vector(w)\n",
        "        # emb += embedding[w]\n",
        "    return emb / len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "c9ac11fe",
      "metadata": {
        "id": "c9ac11fe"
      },
      "outputs": [],
      "source": [
        "def computeGloveSentence(qlist, embedding):\n",
        "    \"\"\"\n",
        "    对每一个句子来构建句子向量\n",
        "    :param qlist:问题列表\n",
        "    :param embedding:word2vec模型\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 对每一个句子进行求均值的embedding\n",
        "    X_w2v = np.zeros((len(qlist), 300))\n",
        "    for i, q in enumerate(qlist):\n",
        "        # 编码每一个问题\n",
        "        X_w2v[i] = computeGloveSentenceEach(q, embedding)\n",
        "        # print(X_w2v)\n",
        "    return X_w2v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "8e0dbbcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0dbbcc",
        "outputId": "af0317fb-c202-4565-cde1-9111c2561d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.03965362  0.24082079 -0.16425829 ... -0.00299822 -0.1400892\n",
            "  -0.22496009]\n",
            " [-0.04005026 -0.0109747  -0.12546748 ...  0.07687889 -0.17839265\n",
            "  -0.09179237]\n",
            " [-0.06666278  0.10480606 -0.07851283 ... -0.0126547  -0.09636211\n",
            "  -0.07799855]\n",
            " ...\n",
            " [-0.03554737  0.07470422  0.00846837 ...  0.02636478 -0.04205069\n",
            "  -0.10601449]\n",
            " [-0.07434175  0.1010364   0.00940921 ...  0.01630229 -0.03028429\n",
            "  -0.20319642]\n",
            " [-0.0262852   0.09887614 -0.03757961 ...  0.05532637 -0.08545647\n",
            "  -0.13127398]]\n"
          ]
        }
      ],
      "source": [
        "# 加载到word2vec模型\n",
        "# 这是 D*H的矩阵，这里的D是词典库的大小， H是词向量的大小。 这里面我们给定的每个单词的词向量，\n",
        "emb = model.wv\n",
        "# 初始化完emb之后就可以对每一个句子来构建句子向量了，这个过程使用average pooling来实现\n",
        "X_w2v = computeGloveSentence(qlist, emb)\n",
        "print(X_w2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "ce5942ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ce5942ad",
        "outputId": "26f4fdb7-e8a0-4761-ba22-f1af57c9a9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert_embedding\n",
            "  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting numpy==1.14.6\n",
            "  Downloading numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 25.1 MB/s \n",
            "\u001b[?25hCollecting gluonnlp==0.6.0\n",
            "  Downloading gluonnlp-0.6.0.tar.gz (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 65.8 MB/s \n",
            "\u001b[?25hCollecting mxnet==1.4.0\n",
            "  Downloading mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting typing==3.6.6\n",
            "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert_embedding) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert_embedding) (2022.5.18.1)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-py3-none-any.whl size=259930 sha256=3ca603b3ade505b40b49441739471cccee3d89b0be0b07381593183885bcbb45\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/41/8f/45bd1c58055d87aee5a71b6756a427ea8d92e506b3a9d17370\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: numpy, graphviz, typing, mxnet, gluonnlp, bert-embedding\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc3 3.11.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 numpy-1.14.6 typing-3.6.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install bert_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd25048f",
      "metadata": {
        "id": "cd25048f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# TODO 基于BERT的句子向量计算\n",
        "from bert_embedding import BertEmbedding\n",
        "\n",
        "sentence_embedding = np.ones((len(qlist), 768))\n",
        "# 加载Bert模型，model，dataset_name,须指定\n",
        "bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='wiki_multilingual_cased')\n",
        "# 查询所有句子的Bert  embedding\n",
        "all_embedding = bert_embedding(qlist, 'sum')\n",
        "for q in qlist:\n",
        "    for i in range(len(all_embedding)):\n",
        "        print(all_embedding[i][1])\n",
        "        # 将每一个句子中每一个词的向量进行拼接求平均得到句子向量\n",
        "        sentence_embedding[i] = np.sum(all_embedding[i][1], axis=0) / len(q.strip().split(' '))\n",
        "        if i == 0:\n",
        "            print(sentence_embedding[i])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f6b648",
      "metadata": {
        "id": "48f6b648"
      },
      "outputs": [],
      "source": [
        "# 每一个句子的向量结果存放在X_bert矩阵里。行数为句子的总个数，列数为一个句子embedding大小。\n",
        "# X_bert = sentence_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "1a47b0e3",
      "metadata": {
        "id": "1a47b0e3"
      },
      "outputs": [],
      "source": [
        "# ### 第三部分： 相似度匹配以及搜索\n",
        "# 在这部分里，我们需要把用户每一个输入跟知识库里的每一个问题做一个相似度计算，从而得出最相似的问题。但对于这个问题，时间复杂度其实很高，所以我们需要结合倒排表来获取相似度最高的问题，从而获得答案。\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# #### 3.1 tf-idf + 余弦相似度\n",
        "# 我们可以直接基于计算出来的``tf-idf``向量，计算用户最新问题与库中存储的问题之间的相似度，从而选择相似度最高的问题的答案。\n",
        "# 这个方法的复杂度为``O(N)``， ``N``是库中问题的个数。\n",
        "\n",
        "import queue as Q\n",
        "\n",
        "# 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
        "que = Q.PriorityQueue()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "48b773ca",
      "metadata": {
        "id": "48b773ca"
      },
      "outputs": [],
      "source": [
        "def cosineSimilarity(vec1, vec2):\n",
        "    # 定义余弦相似度,余弦相似度越大，两个向量越相似\n",
        "    return np.dot(vec1, vec2.T) / (np.sqrt(np.sum(vec1 ** 2)) * np.sqrt(np.sum(vec2 ** 2)))\n",
        "\n",
        "\n",
        "def get_top_results_tfidf_noindex(query):\n",
        "    \"\"\"\n",
        "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
        "    :param query:用户新问题\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
        "    1. 对于用户的输入 query 首先做一系列的预处理(上面提到的方法)，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
        "    2. 计算跟每个库里的问题之间的相似度\n",
        "    3. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"\n",
        "    top = 5\n",
        "    # 将用户输入的新问题用tf-idf来表示\n",
        "    query_tfidf = computeSentenceEach(query.lower(), vectorizer, word2id)\n",
        "    for i, vec in enumerate(X_tfidf):\n",
        "        # 计算原问题与用户输入的新问题的相似度\n",
        "        result = cosineSimilarity(vec, query_tfidf)\n",
        "        # print(result)\n",
        "        # 存放到大顶堆里面\n",
        "        que.put((-1 * result, i))\n",
        "    i = 0\n",
        "    top_idxs = []\n",
        "    while (i < top and not que.empty()):\n",
        "        # top_idxs存放相似度最高的（存在qlist里的）问题的下标\n",
        "        top_idxs.append(que.get()[1])\n",
        "        i += 1\n",
        "    print(top_idxs)\n",
        "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
        "    return np.array(alist)[top_idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "f0f269f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f269f5",
        "outputId": "b05e9539-f928-4582-b45f-0a8db3e99a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2275, 1537, 658, 739, 843]\n",
            "en commerçant ; empruntent des mots français ; Paris ; la prochaine suppression des Chambres de l’édit de Paris et de Rouen ; protestations d'attachement éternel, serments non tenus, engagements réciproques, alliances familiales de même rang, ventes et achats de terrains, marchandages continuels et contrats \n"
          ]
        }
      ],
      "source": [
        "# 给定用户输入的问题 query, 返回最有可能的TOP 5问题的答案。\n",
        "results = get_top_results_tfidf_noindex(\"comment les africains répandent la religion\")\n",
        "print(\" ; \".join(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "c6e3122c",
      "metadata": {
        "id": "c6e3122c"
      },
      "outputs": [],
      "source": [
        "word_doc = dict()\n",
        "# key:word,value:包含该词的句子序号的列表\n",
        "for i, q in enumerate(qlist):\n",
        "    words = q.strip().split(' ')\n",
        "    for w in set(words):\n",
        "        if w not in word_doc:\n",
        "            # 没在word_doc中的，建立一个空listi\n",
        "            word_doc[w] = set([])\n",
        "        word_doc[w] = word_doc[w] | set([i])\n",
        "\n",
        "# 定一个一个简单的倒排表，是一个map结构。 循环所有qlist一遍就可以\n",
        "inverted_idx = word_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "8d4be8d1",
      "metadata": {
        "id": "8d4be8d1"
      },
      "outputs": [],
      "source": [
        "# #### 3.3 语义相似度\n",
        "# 读取语义相关的单词\n",
        "import codecs\n",
        "\n",
        "\n",
        "def get_related_words(filename):\n",
        "    \"\"\"\n",
        "    从预处理的相似词的文件加载相似词信息\n",
        "    文件格式w1 w2 w3..w11,其中w1为原词，w2-w11为w1的相似词\n",
        "    :param filename: 文件名\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    related_words = {}\n",
        "    with codecs.open(filename, 'r', 'utf8') as Fin:\n",
        "        lines = Fin.readlines()\n",
        "    for line in lines:\n",
        "        words = line.strip().split(' ')\n",
        "        # 键为原词，值为相似词\n",
        "        related_words[words[0]] = words[1:]\n",
        "    return related_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18151b2",
      "metadata": {
        "id": "f18151b2"
      },
      "outputs": [],
      "source": [
        "# 从预处理的相似词的文件加载相似词信息\n",
        "related_words = get_related_words('data/related_words.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54e458a",
      "metadata": {
        "id": "e54e458a"
      },
      "outputs": [],
      "source": [
        "# #### 3.4 利用倒排表搜索\n",
        "# 在这里，我们使用倒排表先获得一批候选问题，然后再通过余弦相似度做精准匹配，这样一来可以节省大量的时间。搜索过程分成两步：\n",
        "# - 使用倒排表把候选问题全部提取出来。首先，对输入的新问题做分词等必要的预处理工作，然后对于句子里的每一个单词，从``related_words``里提取出跟它意思相近的top 10单词， 然后根据这些top词从倒排表里提取相关的文档，把所有的文档返回。 这部分可以放在下面的函数当中，也可以放在外部。\n",
        "# - 然后针对于这些文档做余弦相似度的计算，最后排序并选出最好的答案。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "431f40b5",
      "metadata": {
        "id": "431f40b5"
      },
      "outputs": [],
      "source": [
        "import queue as Q\n",
        "\n",
        "\n",
        "def cosineSimilarity(vec1, vec2):\n",
        "    # 定义余弦相似度\n",
        "    return np.dot(vec1, vec2.T) / (np.sqrt(np.sum(vec1 ** 2)) * np.sqrt(np.sum(vec2 ** 2)))\n",
        "\n",
        "\n",
        "def getCandidate(query):\n",
        "    \"\"\"\n",
        "    根据查询句子中每个词及其10个相似词所在的序号列表，求交集\n",
        "    :param query: 问题\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    searched = set()\n",
        "    for w in query.strip().split(' '):\n",
        "        # 如果单词不在word2id中或者不在倒排表中\n",
        "        if w not in word2id or w not in inverted_idx:\n",
        "            continue\n",
        "        # 搜索原词所在的序号列表\n",
        "        if len(searched) == 0:\n",
        "            searched = set(inverted_idx[w])\n",
        "        else:\n",
        "            searched = searched & set(inverted_idx[w])\n",
        "            '''\n",
        "        # 搜索相似词所在的列表\n",
        "        if w in related_words:\n",
        "            for similar in related_words[w]:\n",
        "                searched = searched & set(inverted_idx[similar])\n",
        "                '''\n",
        "    return searched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "0d6983a3",
      "metadata": {
        "id": "0d6983a3"
      },
      "outputs": [],
      "source": [
        "def get_top_results_tfidf(query):\n",
        "    \"\"\"\n",
        "    基于TF-IDF,给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
        "    这里面需要做到以下几点：\n",
        "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
        "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
        "    3. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"\n",
        "    top = 5\n",
        "    # 计算给定句子的TF-IDF\n",
        "    query_tfidf = computeSentenceEach(query, vectorizer, word2id)\n",
        "    # 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
        "    results = Q.PriorityQueue()\n",
        "    # 利用倒排索引表获取相似问题\n",
        "    searched = getCandidate(query)\n",
        "    # print(len(searched))\n",
        "    for candidate in searched:\n",
        "        # 计算candidate与query的余弦相似度\n",
        "        result = cosineSimilarity(query_tfidf, X_tfidf[candidate])\n",
        "        # 优先级队列中保存相似度和对应的candidate序号\n",
        "        # -1保证降序\n",
        "        results.put((-1 * result, candidate))\n",
        "    i = 0\n",
        "    # top_idxs存放相似度最高的（存在qlist里的）问题的索引\n",
        "    top_idxs = []\n",
        "    # hint: 利用priority queue来找出top results.\n",
        "    while i < top and not results.empty():\n",
        "        top_idxs.append(results.get()[1])\n",
        "        i += 1\n",
        "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
        "    return np.array(alist)[top_idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "75ab428e",
      "metadata": {
        "id": "75ab428e"
      },
      "outputs": [],
      "source": [
        "def get_top_results_w2v(query):\n",
        "    \"\"\"\n",
        "    基于word2vec，给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
        "    这里面需要做到以下几点：\n",
        "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
        "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
        "    3. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"\n",
        "    # embedding用glove\n",
        "    top = 5\n",
        "    # 利用glove将问题进行编码\n",
        "    query_emb = computeGloveSentenceEach(query, emb)\n",
        "    # 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
        "    results = Q.PriorityQueue()\n",
        "    # 利用倒排索引表获取相似问题\n",
        "    searched = getCandidate(query)\n",
        "    for candidate in searched:\n",
        "        # 计算candidate与query的余弦相似度\n",
        "        result = cosineSimilarity(query_emb, X_w2v[candidate])\n",
        "        # 优先级队列中保存相似度和对应的candidate序号\n",
        "        # -1保证降序\n",
        "        results.put((-1 * result, candidate))\n",
        "    # top_idxs存放相似度最高的（存在qlist里的）问题的索引\n",
        "    top_idxs = []\n",
        "    i = 0\n",
        "    # hint: 利用priority queue来找出top results.\n",
        "    while i < top and not results.empty():\n",
        "        top_idxs.append(results.get()[1])\n",
        "        i += 1\n",
        "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
        "    return np.array(alist)[top_idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "e8c98a42",
      "metadata": {
        "id": "e8c98a42"
      },
      "outputs": [],
      "source": [
        "def get_top_results_bert(query):\n",
        "    \"\"\"\n",
        "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
        "    这里面需要做到以下几点：\n",
        "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
        "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
        "    3. 找出相似度最高的top5问题的答案\n",
        "    \"\"\"\n",
        "    top = 5\n",
        "    # embedding用Bert embedding\n",
        "    query_emb = np.sum(bert_embedding([query], 'sum')[0][1], axis=0) / len(query.strip().split())\n",
        "    # 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
        "    results = Q.PriorityQueue()\n",
        "    # 利用倒排索引表获取相似问题\n",
        "    searched = getCandidate(query)\n",
        "    for candidate in searched:\n",
        "        # 计算candidate与query的余弦相似度\n",
        "        result = cosineSimilarity(query_emb, X_bert[candidate])\n",
        "        # 优先级队列中保存相似度和对应的candidate序号\n",
        "        # -1保证降序\n",
        "        results.put((-1 * result, candidate))\n",
        "    # top_idxs存放相似度最高的（存在qlist里的）问题的索引\n",
        "    top_idxs = []\n",
        "    i = 0\n",
        "    # hint: 利用priority queue来找出top results.\n",
        "    while i < top and not results.empty():\n",
        "        top_idxs.append(results.get()[1])\n",
        "        i += 1\n",
        "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
        "    return np.array(alist)[top_idxs]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDcVtOLksPSa",
        "outputId": "6dabe4bf-dcac-44ba-95e5-cc756167b335"
      },
      "id": "xDcVtOLksPSa",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 34.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# correcteur en français avec spellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='fr')"
      ],
      "metadata": {
        "id": "2pxoLdIQtUZZ"
      },
      "id": "2pxoLdIQtUZZ",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "8aed31ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aed31ad",
        "outputId": "7ee5fe86-5f44-4f04-888b-61d96004520d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "la question correcte :  comment les africains répandent la religion\n",
            "[2275, 1537, 8337, 8552, 8559]\n",
            "tfidf_noindex :  en commerçant ; empruntent des mots français ; historienne des mathématiques ; vol ; ingénierie de fiabilité\n",
            "tf-idf :  en commerçant\n",
            "word2vec :  en commerçant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        }
      ],
      "source": [
        "# la quesiton mal orthographié\n",
        "query = 'commnet les afrcains répandent la religoin'\n",
        "# tokens\n",
        "misspelled = query.split(' ')\n",
        "# correcter mot par mot\n",
        "correcte = [str(spell.correction(word)) for word in misspelled]\n",
        "# retour la question à l'orthographe correcte\n",
        "query = \" \".join(correcte)\n",
        "print(\"la question correcte : \",query)\n",
        "\n",
        "results_tfidf_noindex = get_top_results_tfidf_noindex(query)\n",
        "results_tfidf = get_top_results_tfidf(query)\n",
        "results_w2v = get_top_results_w2v(query)\n",
        "# results_bert = get_top_results_bert(query)\n",
        "\n",
        "print(\"tfidf_noindex : \",\" ; \".join(results_tfidf_noindex))\n",
        "print(\"tf-idf : \",\" ; \".join(results_tfidf))\n",
        "print(\"word2vec : \",\" ; \".join(results_w2v))\n",
        "# print(\"bert : \",\" ; \".join(results_bert))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "AQ.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}