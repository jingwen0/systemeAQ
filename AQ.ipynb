{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c554db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ### 第一部分：对于训练数据的处理：读取文件和预处理\n",
    "\n",
    "# - ```文本的读取```： 需要从文本中读取数据，此处需要读取的文件是```dev-v2.0.json```，并把读取的文件存入一个列表里（list）\n",
    "# - ```文本预处理```： 对于问题本身需要做一些停用词过滤等文本方面的处理\n",
    "# - ```可视化分析```： 对于给定的样本数据，做一些可视化分析来更好地理解数据\n",
    "\n",
    "\n",
    "# #### 1.1节： 文本的读取\n",
    "# 把给定的文本数据读入到```qlist```和```alist```当中，这两个分别是列表，其中```qlist```是问题的列表，```alist```是对应的答案列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9b4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 1.1节： 文本的读取\n",
    "# 把给定的文本数据读入到```qlist```和```alist```当中，这两个分别是列表，其中```qlist```是问题的列表，```alist```是对应的答案列表\n",
    "def read_corpus():\n",
    "    \"\"\"\n",
    "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
    "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
    "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
    "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
    "    \"\"\"\n",
    "    # 问题列表\n",
    "    qlist = []\n",
    "    # 答案列表\n",
    "    alist = []\n",
    "    # 文件名称\n",
    "    filename = 'data/train-v2.0.json'\n",
    "    # 加载json文件\n",
    "    datas = json.load(open(filename, 'r'))\n",
    "    data = datas['data']\n",
    "    for d in data:\n",
    "        paragraph = d['paragraphs']\n",
    "        for p in paragraph:\n",
    "            qas = p['qas']\n",
    "            for qa in qas:\n",
    "                # 处理is_impossible为True时answers空\n",
    "                if (not qa['is_impossible']):\n",
    "                    qlist.append(qa['question'])\n",
    "                    alist.append(qa['answers'][0]['text'])\n",
    "    # 如果它的条件返回错误，则终止程序执行\n",
    "    # 这行代码是确保长度一样\n",
    "    assert len(qlist) == len(alist)\n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d04bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qlist的单词统计： 57807\n"
     ]
    }
   ],
   "source": [
    "# 读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist\n",
    "qlist, alist = read_corpus()\n",
    "\n",
    "# ### 1.2 理解数据（可视化分析/统计信息）\n",
    "# 统计一下在qlist中总共出现了多少个单词？ 总共出现了多少个不同的单词(unique word)？\n",
    "# 这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
    "words_qlist = dict()\n",
    "for q in qlist:\n",
    "    # 以空格为分词，都转为小写\n",
    "    words = q.strip().split(' ')\n",
    "    for w in words:\n",
    "        if w.lower() in words_qlist:\n",
    "            words_qlist[w.lower()] += 1\n",
    "        else:\n",
    "            words_qlist[w.lower()] = 1\n",
    "word_total = len(words_qlist)\n",
    "print(\"qlist的单词统计：\",word_total)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e44748a",
   "metadata": {},
   "source": [
    "# 统计一下qlist中出现1次，2次，3次... 出现的单词个数， 然后画一个plot. 这里的x轴是单词出现的次数（1，2，3，..)， y轴是单词个数。\n",
    "# 从左到右分别是 出现1次的单词数，出现2次的单词数，出现3次的单词数\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# counts：key出现N次，value：出现N次词有多少\n",
    "counts = dict()\n",
    "for w, c in words_qlist.items():\n",
    "    if c in counts:\n",
    "        counts[c] += 1\n",
    "    else:\n",
    "        counts[c] = 1\n",
    "# 以histogram画图\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(counts.values(), bins=np.arange(0, 250, 25), histtype='step', alpha=0.6, label=\"counts\")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 250)\n",
    "ax.set_yticks(np.arange(0, 500, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e8a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 1.3 文本预处理\n",
    "# 此部分需要做文本方面的处理。 以下是可以用到的一些方法：\n",
    "#\n",
    "# - 1. 停用词过滤 （去网上搜一下 \"english stop words list\"，会出现很多包含停用词库的网页，或者直接使用NLTK自带的）\n",
    "# - 2. 转换成lower_case： 这是一个基本的操作\n",
    "# - 3. 去掉一些无用的符号： 比如连续的感叹号！！！， 或者一些奇怪的单词。\n",
    "# - 4. 去掉出现频率很低的词：比如出现次数少于10,20.... （想一下如何选择阈值）\n",
    "# - 5. 对于数字的处理： 分词完只有有些单词可能就是数字比如44，415，把所有这些数字都看成是一个单词，这个新的单词我们可以定义为 \"#number\"\n",
    "# - 6. lemmazation： 在这里不要使用stemming， 因为stemming的结果有可能不是valid word。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f9e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.16.4\n",
      "  Using cached numpy-1.16.4-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (13.9 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.19.0.dev0 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n",
      "thinc 8.0.13 requires typing-extensions<4.0.0.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\n",
      "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.16.4 which is incompatible.\n",
      "statsmodels 0.13.2 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n",
      "spacy 3.2.3 requires typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.1.1 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.16.4 which is incompatible.\n",
      "scikit-image 0.19.2 requires numpy>=1.17.0, but you have numpy 1.16.4 which is incompatible.\n",
      "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.4 which is incompatible.\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.16.4 which is incompatible.\n",
      "pyerfa 2.0.0 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n",
      "pyensae 1.3.884 requires numpy>=1.18, but you have numpy 1.16.4 which is incompatible.\n",
      "pandas 1.3.5 requires numpy>=1.17.3, but you have numpy 1.16.4 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.16.4 which is incompatible.\n",
      "mxnet 1.4.0 requires numpy<1.15.0,>=1.8.2, but you have numpy 1.16.4 which is incompatible.\n",
      "matplotlib 3.5.2 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n",
      "librosa 0.9.1 requires numpy>=1.17.0, but you have numpy 1.16.4 which is incompatible.\n",
      "en-core-web-sm 3.0.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 3.2.3 which is incompatible.\n",
      "bert-embedding 1.0.1 requires numpy==1.14.6, but you have numpy 1.16.4 which is incompatible.\n",
      "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.16.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.16.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06912d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/__init__.py:149: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.16.4\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "# 去掉一些无用的符号\n",
    "def tokenizer(ori_list):\n",
    "    # 利用正则表达式去掉无用的符号\n",
    "    # compile 函数用于编译正则表达式，[]用来表示一组字符\n",
    "    # \\s匹配任意空白字符，等价于 [\\t\\n\\r\\f]。\n",
    "    SYMBOLS = re.compile('[\\s;\\\"\\\",.!?\\\\/\\[\\]\\{\\}\\(\\)-]+')\n",
    "    new_list = []\n",
    "    for q in ori_list:\n",
    "        # split 方法按照能够匹配的子串将字符串分割后返回列表\n",
    "        words = SYMBOLS.split(q.lower().strip())\n",
    "        new_list.append(' '.join(words))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b736cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉question的停用词\n",
    "def removeStopWord(ori_list):\n",
    "    new_list = []\n",
    "    # nltk中stopwords包含what等，但是在QA问题中，这算关键词，所以不看作关键词\n",
    "    restored = ['what', 'when', 'which', 'how', 'who', 'where']\n",
    "    # nltk中自带的停用词库\n",
    "    english_stop_words = list(\n",
    "        set(stopwords.words('english')))  # ['what','when','which','how','who','where','a','an','the']\n",
    "    # 将在QA问答系统中不算停用词的词去掉\n",
    "    for w in restored:\n",
    "        english_stop_words.remove(w)\n",
    "    for q in ori_list:\n",
    "        # 将每个问句的停用词去掉\n",
    "        sentence = ' '.join([w for w in q.strip().split(' ') if w not in english_stop_words])\n",
    "        # 将去掉停用词的问句添加至列表中\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a45a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLowFrequence(ori_list, vocabulary, thres=10):\n",
    "    \"\"\"\n",
    "    去掉低频率的词\n",
    "    :param ori_list: 预处理后的问题列表\n",
    "    :param vocabulary: 词频率字典\n",
    "    :param thres: 频率阈值，可以基于数据实际情况进行调整\n",
    "    :return: 新的问题列表\n",
    "    \"\"\"\n",
    "    # 根据thres筛选词表，小于thres的词去掉\n",
    "    new_list = []\n",
    "    for q in ori_list:\n",
    "        sentence = ' '.join([w for w in q.strip().split(' ') if vocabulary[w] >= thres])\n",
    "        new_list.append(sentence)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b70a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceDigits(ori_list, replace='#number'):\n",
    "    \"\"\"\n",
    "    将数字统一替换为replace,默认#number\n",
    "    :param ori_list: 预处理后的问题列表\n",
    "    :param replace:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 编译正则表达式：匹配1个或多个数字\n",
    "    DIGITS = re.compile('\\d+')\n",
    "    new_list = []\n",
    "    for q in ori_list:\n",
    "        # re.sub用于替换字符串中的匹配项，相当于在q中查找连续的数字替换为#number\n",
    "        q = DIGITS.sub(replace, q)\n",
    "        # 将处理后的问题字符串添加到新列表中\n",
    "        new_list.append(q)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b1ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocab(ori_list):\n",
    "    \"\"\"\n",
    "    创建词表，统计所有单词总数与每个单词总数\n",
    "    :param ori_list:预处理后的列表\n",
    "    :return:所有单词总数与每个单词总数\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    vocab_count = dict()\n",
    "    for q in ori_list:\n",
    "        words = q.strip().split(' ')\n",
    "        count += len(words)\n",
    "        for w in words:\n",
    "            if w in vocab_count:\n",
    "                vocab_count[w] += 1\n",
    "            else:\n",
    "                vocab_count[w] = 1\n",
    "    return vocab_count, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76aaad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFile(oriList, filename):\n",
    "    \"\"\"\n",
    "    将处理后的问题列表写入到文件中\n",
    "    :param oriList: 预处理后的问题列表\n",
    "    :param filename: 文件名\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, 'w', 'utf8') as Fout:\n",
    "        for q in oriList:\n",
    "            Fout.write(q + u'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b17bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeVocab(vocabulary, filename):\n",
    "    \"\"\"\n",
    "    将词表写入到文件中\n",
    "    :param vocabulary: 词表\n",
    "    :param filename: 文件名\n",
    "    \"\"\"\n",
    "    sortedList = sorted(vocabulary.items(), key=lambda d: d[1])\n",
    "    with codecs.open(filename, 'w', 'utf8') as Fout:\n",
    "        for (w, c) in sortedList:\n",
    "            Fout.write(w + u':' + str(c) + u'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e826473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉一些无用的符号\n",
    "new_list = tokenizer(qlist)\n",
    "# 停用词过滤\n",
    "new_list = removeStopWord(new_list)\n",
    "# 数字处理-将数字替换为#number\n",
    "new_list = replaceDigits(new_list)\n",
    "# 创建词表并统计所有单词数目\n",
    "vocabulary, count = createVocab(new_list)\n",
    "# 去掉低频率的词\n",
    "new_list = removeLowFrequence(new_list, vocabulary, 5)\n",
    "# 重新统计词频\n",
    "vocab_count, count = createVocab(new_list)\n",
    "# 将词表写入到文件“train.vocab”中\n",
    "writeVocab(vocab_count, \"train.vocab\")\n",
    "qlist = new_list\n",
    "# print(qlist[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fcf773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 第二部分： 文本的表示\n",
    "# 文本处理完之后，我们需要做文本表示，这里有3种方式：\n",
    "#\n",
    "# - 1. 使用```tf-idf vector```\n",
    "# - 2. 使用embedding技术如```word2vec```, ```bert embedding```等\n",
    "\n",
    "# #### 2.1 使用tf-idf表示向量\n",
    "# 把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里。\n",
    "# # ``X``的大小是： ``N* D``的矩阵。 这里``N``是问题的个数（样本个数），``D``是词典库的大小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa5efc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (1.16.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a13b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def computeTF(vocab, c):\n",
    "    \"\"\"\n",
    "    计算每个词的词频\n",
    "    :param vocab: 词频字典:键是单词，值是所有问句中单词出现的次数\n",
    "    :param c: 单词总数\n",
    "    :return: TF\n",
    "    \"\"\"\n",
    "    # 初始化TF\n",
    "    TF = np.ones(len(vocab))\n",
    "    # 词频字典\n",
    "    word2id = dict()\n",
    "    # 单词字典\n",
    "    id2word = dict()\n",
    "    for word, fre in vocab.items():\n",
    "        # 计算TF值：每个单词出现的个数/总的单词个数\n",
    "        TF[len(word2id)] = 1.0 * fre / c\n",
    "        id2word[len(word2id)] = word\n",
    "        word2id[word] = len(word2id)\n",
    "    return TF, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b97aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(word2id, qlist):\n",
    "    \"\"\"\n",
    "    计算IDF：log[问句总数/(包含单词t的问句总数+1)]\n",
    "    :param word2id:单词字典\n",
    "    :param qlist:问句列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    IDF = np.ones(len(word2id))\n",
    "    for q in qlist:\n",
    "        # 去重\n",
    "        words = set(q.strip().split())\n",
    "        for w in words:\n",
    "            # 统计单词出现在问句中的总数\n",
    "            IDF[word2id[w]] += 1\n",
    "    # 计算IDF\n",
    "    IDF /= len(qlist)\n",
    "    IDF = -1.0 * np.log2(IDF)\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3c405e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSentenceEach(sentence, tfidf, word2id):\n",
    "    \"\"\"\n",
    "    给定句子，计算句子TF-IDF,tfidf是一个1*M的矩阵,M为词表大小\n",
    "    :param sentence:句子\n",
    "    :param tfidf:TF-IDF向量\n",
    "    :param word2id:词表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence_tfidf = np.zeros(len(word2id))\n",
    "    # 将问句以空格进行分割\n",
    "    for w in sentence.strip().split(' '):\n",
    "        if w not in word2id:\n",
    "            continue\n",
    "        # 碰到在词表word2id中的单词\n",
    "        sentence_tfidf[word2id[w]] = tfidf[word2id[w]]\n",
    "    return sentence_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f31f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSentence(qlist, word2id, tfidf):\n",
    "    \"\"\"\n",
    "    把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里\n",
    "    :param qlist: 问题列表\n",
    "    :param word2id: 词表(字典形式)\n",
    "    :param tfidf: TF-IDF(与词表中的键一一对应)\n",
    "    :return: X矩阵\n",
    "    \"\"\"\n",
    "    # 对所有句子分别求tfidf\n",
    "    X_tfidf = np.zeros((len(qlist), len(word2id)))\n",
    "    for i, q in enumerate(qlist):\n",
    "        X_tfidf[i] = computeSentenceEach(q, tfidf, word2id)\n",
    "        # print(X_tfidf[i])\n",
    "    return X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae023d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个词的TF,词表字典\n",
    "TF, word2id, id2word = computeTF(vocab_count, count)\n",
    "# 计算IDF：log[问句总数/(包含单词t的问句总数+1)]\n",
    "IDF = computeIDF(word2id, qlist)\n",
    "# 用TF，IDF计算最终的tf-idf,定义一个tf-idf的vectorizer\n",
    "vectorizer = np.multiply(TF, IDF)\n",
    "# 把```qlist```中的每一个问题的字符串转换成```tf-idf```向量, 转换之后的结果存储在```X```矩阵里\n",
    "X_tfidf = computeSentence(qlist, word2id, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dfbdcff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from python-Levenshtein) (61.2.0)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/anaconda3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-wheel-levrh52h\n",
      "       cwd: /private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/\n",
      "  Complete output (139 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-x86_64-3.7\n",
      "  creating build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "  copying Levenshtein/StringMatcher.py -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "  copying Levenshtein/__init__.py -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "  running egg_info\n",
      "  writing python_Levenshtein.egg-info/PKG-INFO\n",
      "  writing dependency_links to python_Levenshtein.egg-info/dependency_links.txt\n",
      "  deleting python_Levenshtein.egg-info/entry_points.txt\n",
      "  writing namespace_packages to python_Levenshtein.egg-info/namespace_packages.txt\n",
      "  writing requirements to python_Levenshtein.egg-info/requires.txt\n",
      "  writing top-level names to python_Levenshtein.egg-info/top_level.txt\n",
      "  reading manifest file 'python_Levenshtein.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*pyc' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*so' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.project' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.pydevproject' found anywhere in distribution\n",
      "  adding license file 'COPYING'\n",
      "  writing manifest file 'python_Levenshtein.egg-info/SOURCES.txt'\n",
      "  copying Levenshtein/_levenshtein.c -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "  copying Levenshtein/_levenshtein.h -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "  running build_ext\n",
      "  building 'Levenshtein._levenshtein' extension\n",
      "  creating build/temp.macosx-10.9-x86_64-3.7\n",
      "  creating build/temp.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "  x86_64-apple-darwin13.4.0-clang -fno-strict-aliasing -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -pipe -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=/opt/anaconda3=/usr/local/src/conda-prefix -flto -Wl,-export_dynamic -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /opt/anaconda3/include -D_FORTIFY_SOURCE=2 -isystem /opt/anaconda3/include -I/opt/anaconda3/include/python3.7m -c Levenshtein/_levenshtein.c -o build/temp.macosx-10.9-x86_64-3.7/Levenshtein/_levenshtein.o\n",
      "  clang-12: warning: -Wl,-export_dynamic: 'linker' input unused [-Wunused-command-line-argument]\n",
      "  Levenshtein/_levenshtein.c:731:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:732:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:816:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:817:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:860:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:861:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:910:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:911:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1012:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        result = PyString_FromStringAndSize(medstr, len);\n",
      "                                            ^~~~~~\n",
      "  /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "  PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                               ^\n",
      "  Levenshtein/_levenshtein.c:1091:15: warning: initializing 'lev_byte *' (aka 'unsigned char *') with an expression of type 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      lev_byte *s = PyString_AS_STRING(arg1);\n",
      "                ^   ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1097:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        result = PyString_FromStringAndSize(medstr, len);\n",
      "                                            ^~~~~~\n",
      "  /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "  PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                               ^\n",
      "  Levenshtein/_levenshtein.c:1135:41: warning: comparison of integers of different signs: 'long' and 'size_t' (aka 'unsigned long') [-Wsign-compare]\n",
      "      if (PySequence_Fast_GET_SIZE(wlist) != n) {\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^  ~\n",
      "  Levenshtein/_levenshtein.c:1221:16: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      strings[0] = PyString_AS_STRING(first);\n",
      "                 ^ ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1233:18: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        strings[i] = PyString_AS_STRING(item);\n",
      "                   ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1399:13: warning: unused variable 'len' [-Wunused-variable]\n",
      "    size_t i, len;\n",
      "              ^\n",
      "  Levenshtein/_levenshtein.c:1398:15: warning: unused variable 's' [-Wunused-variable]\n",
      "    const char *s;\n",
      "                ^\n",
      "  Levenshtein/_levenshtein.c:1670:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1671:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1788:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1789:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1883:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string1 = PyString_AS_STRING(arg1);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1884:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "      string2 = PyString_AS_STRING(arg2);\n",
      "              ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  Levenshtein/_levenshtein.c:1898:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        result = PyString_FromStringAndSize(s, len);\n",
      "                                            ^\n",
      "  /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "  PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                               ^\n",
      "  Levenshtein/_levenshtein.c:1914:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        result = PyString_FromStringAndSize(s, len);\n",
      "                                            ^\n",
      "  /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "  PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                               ^\n",
      "  Levenshtein/_levenshtein.c:2080:27: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int' [-Wsign-compare]\n",
      "            if (!orem && nr == -1) {\n",
      "                         ~~ ^  ~~\n",
      "  Levenshtein/_levenshtein.c:6650:1: warning: unused function 'lev_editops_total_cost' [-Wunused-function]\n",
      "  lev_editops_total_cost(size_t n,\n",
      "  ^\n",
      "  Levenshtein/_levenshtein.c:6720:1: warning: unused function 'lev_opcodes_total_cost' [-Wunused-function]\n",
      "  lev_opcodes_total_cost(size_t nb,\n",
      "  ^\n",
      "  Levenshtein/_levenshtein.c:6675:1: warning: unused function 'lev_editops_normalize' [-Wunused-function]\n",
      "  lev_editops_normalize(size_t n,\n",
      "  ^\n",
      "  Levenshtein/_levenshtein.c:2391:1: warning: unused function 'lev_edit_distance_sod' [-Wunused-function]\n",
      "  lev_edit_distance_sod(size_t len, const lev_byte *string,\n",
      "  ^\n",
      "  Levenshtein/_levenshtein.c:2570:1: warning: unused function 'lev_u_edit_distance_sod' [-Wunused-function]\n",
      "  lev_u_edit_distance_sod(size_t len, const lev_wchar *string,\n",
      "  ^\n",
      "  30 warnings generated.\n",
      "  x86_64-apple-darwin13.4.0-clang -bundle -undefined dynamic_lookup -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/anaconda3/lib -L/opt/anaconda3/lib -flto -Wl,-export_dynamic -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/anaconda3/lib -L/opt/anaconda3/lib -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/anaconda3/lib -L/opt/anaconda3/lib -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /opt/anaconda3/include -D_FORTIFY_SOURCE=2 -isystem /opt/anaconda3/include build/temp.macosx-10.9-x86_64-3.7/Levenshtein/_levenshtein.o -o build/lib.macosx-10.9-x86_64-3.7/Levenshtein/_levenshtein.cpython-37m-darwin.so\n",
      "  ld: warning: -pie being ignored. It is only used when linking a main executable\n",
      "  ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX12.3.sdk/usr/lib/libSystem.tbd' for architecture x86_64\n",
      "  clang-12: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  error: command '/opt/anaconda3/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for python-Levenshtein\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for python-Levenshtein\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "    Running setup.py install for python-Levenshtein ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/anaconda3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-record-67agxqkn/install-record.txt --single-version-externally-managed --compile --install-headers /opt/anaconda3/include/python3.7m/python-Levenshtein\n",
      "         cwd: /private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/\n",
      "    Complete output (140 lines):\n",
      "    running install\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      setuptools.SetuptoolsDeprecationWarning,\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.9-x86_64-3.7\n",
      "    creating build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "    copying Levenshtein/StringMatcher.py -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "    copying Levenshtein/__init__.py -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "    running egg_info\n",
      "    writing python_Levenshtein.egg-info/PKG-INFO\n",
      "    writing dependency_links to python_Levenshtein.egg-info/dependency_links.txt\n",
      "    writing namespace_packages to python_Levenshtein.egg-info/namespace_packages.txt\n",
      "    writing requirements to python_Levenshtein.egg-info/requires.txt\n",
      "    writing top-level names to python_Levenshtein.egg-info/top_level.txt\n",
      "    reading manifest file 'python_Levenshtein.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '*pyc' found anywhere in distribution\n",
      "    warning: no previously-included files matching '*so' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.project' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.pydevproject' found anywhere in distribution\n",
      "    adding license file 'COPYING'\n",
      "    writing manifest file 'python_Levenshtein.egg-info/SOURCES.txt'\n",
      "    copying Levenshtein/_levenshtein.c -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "    copying Levenshtein/_levenshtein.h -> build/lib.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "    running build_ext\n",
      "    building 'Levenshtein._levenshtein' extension\n",
      "    creating build/temp.macosx-10.9-x86_64-3.7\n",
      "    creating build/temp.macosx-10.9-x86_64-3.7/Levenshtein\n",
      "    x86_64-apple-darwin13.4.0-clang -fno-strict-aliasing -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -pipe -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=/opt/anaconda3=/usr/local/src/conda-prefix -flto -Wl,-export_dynamic -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /opt/anaconda3/include -D_FORTIFY_SOURCE=2 -isystem /opt/anaconda3/include -I/opt/anaconda3/include/python3.7m -c Levenshtein/_levenshtein.c -o build/temp.macosx-10.9-x86_64-3.7/Levenshtein/_levenshtein.o\n",
      "    clang-12: warning: -Wl,-export_dynamic: 'linker' input unused [-Wunused-command-line-argument]\n",
      "    Levenshtein/_levenshtein.c:731:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:732:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:816:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:817:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:860:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:861:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:910:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:911:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1012:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "          result = PyString_FromStringAndSize(medstr, len);\n",
      "                                              ^~~~~~\n",
      "    /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "    PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                                 ^\n",
      "    Levenshtein/_levenshtein.c:1091:15: warning: initializing 'lev_byte *' (aka 'unsigned char *') with an expression of type 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        lev_byte *s = PyString_AS_STRING(arg1);\n",
      "                  ^   ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1097:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "          result = PyString_FromStringAndSize(medstr, len);\n",
      "                                              ^~~~~~\n",
      "    /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "    PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                                 ^\n",
      "    Levenshtein/_levenshtein.c:1135:41: warning: comparison of integers of different signs: 'long' and 'size_t' (aka 'unsigned long') [-Wsign-compare]\n",
      "        if (PySequence_Fast_GET_SIZE(wlist) != n) {\n",
      "            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^  ~\n",
      "    Levenshtein/_levenshtein.c:1221:16: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        strings[0] = PyString_AS_STRING(first);\n",
      "                   ^ ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1233:18: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "          strings[i] = PyString_AS_STRING(item);\n",
      "                     ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1399:13: warning: unused variable 'len' [-Wunused-variable]\n",
      "      size_t i, len;\n",
      "                ^\n",
      "    Levenshtein/_levenshtein.c:1398:15: warning: unused variable 's' [-Wunused-variable]\n",
      "      const char *s;\n",
      "                  ^\n",
      "    Levenshtein/_levenshtein.c:1670:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1671:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1788:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1789:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1883:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string1 = PyString_AS_STRING(arg1);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1884:13: warning: assigning to 'lev_byte *' (aka 'unsigned char *') from 'char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "        string2 = PyString_AS_STRING(arg2);\n",
      "                ^ ~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Levenshtein/_levenshtein.c:1898:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "          result = PyString_FromStringAndSize(s, len);\n",
      "                                              ^\n",
      "    /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "    PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                                 ^\n",
      "    Levenshtein/_levenshtein.c:1914:43: warning: passing 'lev_byte *' (aka 'unsigned char *') to parameter of type 'const char *' converts between pointers to integer types where one is of the unique plain 'char' type and the other is not [-Wpointer-sign]\n",
      "          result = PyString_FromStringAndSize(s, len);\n",
      "                                              ^\n",
      "    /opt/anaconda3/include/python3.7m/bytesobject.h:51:62: note: passing argument to parameter here\n",
      "    PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n",
      "                                                                 ^\n",
      "    Levenshtein/_levenshtein.c:2080:27: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int' [-Wsign-compare]\n",
      "              if (!orem && nr == -1) {\n",
      "                           ~~ ^  ~~\n",
      "    Levenshtein/_levenshtein.c:6650:1: warning: unused function 'lev_editops_total_cost' [-Wunused-function]\n",
      "    lev_editops_total_cost(size_t n,\n",
      "    ^\n",
      "    Levenshtein/_levenshtein.c:6720:1: warning: unused function 'lev_opcodes_total_cost' [-Wunused-function]\n",
      "    lev_opcodes_total_cost(size_t nb,\n",
      "    ^\n",
      "    Levenshtein/_levenshtein.c:6675:1: warning: unused function 'lev_editops_normalize' [-Wunused-function]\n",
      "    lev_editops_normalize(size_t n,\n",
      "    ^\n",
      "    Levenshtein/_levenshtein.c:2391:1: warning: unused function 'lev_edit_distance_sod' [-Wunused-function]\n",
      "    lev_edit_distance_sod(size_t len, const lev_byte *string,\n",
      "    ^\n",
      "    Levenshtein/_levenshtein.c:2570:1: warning: unused function 'lev_u_edit_distance_sod' [-Wunused-function]\n",
      "    lev_u_edit_distance_sod(size_t len, const lev_wchar *string,\n",
      "    ^\n",
      "    30 warnings generated.\n",
      "    x86_64-apple-darwin13.4.0-clang -bundle -undefined dynamic_lookup -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/anaconda3/lib -L/opt/anaconda3/lib -flto -Wl,-export_dynamic -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/anaconda3/lib -L/opt/anaconda3/lib -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/anaconda3/lib -L/opt/anaconda3/lib -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /opt/anaconda3/include -D_FORTIFY_SOURCE=2 -isystem /opt/anaconda3/include build/temp.macosx-10.9-x86_64-3.7/Levenshtein/_levenshtein.o -o build/lib.macosx-10.9-x86_64-3.7/Levenshtein/_levenshtein.cpython-37m-darwin.so\n",
      "    ld: warning: -pie being ignored. It is only used when linking a main executable\n",
      "    ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX12.3.sdk/usr/lib/libSystem.tbd' for architecture x86_64\n",
      "    clang-12: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "    error: command '/opt/anaconda3/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /opt/anaconda3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-install-637_iqt_/python-levenshtein_4ef2686472114fffa8fcd5e246e2546d/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/5m/hbh9nnpn5y92cglh1hvnxmc40000gn/T/pip-record-67agxqkn/install-record.txt --single-version-externally-managed --compile --install-headers /opt/anaconda3/include/python3.7m/python-Levenshtein Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "086376e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.random' has no attribute 'default_rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0cb801b2d471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 国外网址如果很慢，可以在百度上搜索国内服务器上的。 每个词向量获取完之后，即可以得到一个句子的向量。 我们通过``average pooling``来实现句子的向量。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 基于Glove向量获取句子向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove2word2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#: A default, shared numpy-Generator-based PRNG for any/all uses that don't require seeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mdefault_prng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.random' has no attribute 'default_rng'"
     ]
    }
   ],
   "source": [
    "# #### 2.2 使用wordvec + average pooling\n",
    "# 词向量方面需要下载： https://nlp.stanford.edu/projects/glove/ （请下载``glove.6B.zip``），并使用``d=200``的词向量（200维）。\n",
    "# 国外网址如果很慢，可以在百度上搜索国内服务器上的。 每个词向量获取完之后，即可以得到一个句子的向量。 我们通过``average pooling``来实现句子的向量。\n",
    "# 基于Glove向量获取句子向量\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21572ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbedding(filename):\n",
    "    \"\"\"\n",
    "    加载glove模型，转化为word2vec，再加载到word2vec模型\n",
    "    这两种模型形式上是一样的，在数据的保存形式上有略微的差异\n",
    "    :param filename: glove文件\n",
    "    :return: word2vec模型\n",
    "    \"\"\"\n",
    "    word2vec_temp_file = 'word2vec_temp.txt'\n",
    "    # 加载glove模型，转化为word2vec\n",
    "    glove2word2vec(filename, word2vec_temp_file)\n",
    "    # 再加载到word2vec模型\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_temp_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGloveSentenceEach(sentence, embedding):\n",
    "    \"\"\"\n",
    "    :param sentence:问题\n",
    "    :param embedding:wordvec模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 查找句子中每个词的embedding,将所有embedding进行加和求均值\n",
    "    emb = np.zeros(200)\n",
    "    words = sentence.strip().split(' ')\n",
    "    for w in words:\n",
    "        # 如果单词是新词，则重命名为'unknown'\n",
    "        if w not in embedding:\n",
    "            # 没有lookup的即为unknown\n",
    "            w = 'unknown'\n",
    "        # 将所有embedding进行加和求均值\n",
    "        emb += embedding.get_vector(w)\n",
    "        # emb += embedding[w]\n",
    "    return emb / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGloveSentence(qlist, embedding):\n",
    "    \"\"\"\n",
    "    对每一个句子来构建句子向量\n",
    "    :param qlist:问题列表\n",
    "    :param embedding:word2vec模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 对每一个句子进行求均值的embedding\n",
    "    X_w2v = np.zeros((len(qlist), 200))\n",
    "    for i, q in enumerate(qlist):\n",
    "        # 编码每一个问题\n",
    "        X_w2v[i] = computeGloveSentenceEach(q, embedding)\n",
    "        # print(X_w2v)\n",
    "    return X_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载到word2vec模型\n",
    "# 这是 D*H的矩阵，这里的D是词典库的大小， H是词向量的大小。 这里面我们给定的每个单词的词向量，\n",
    "emb = loadEmbedding('data/glove.6B.200d.txt')\n",
    "# 初始化完emb之后就可以对每一个句子来构建句子向量了，这个过程使用average pooling来实现\n",
    "X_w2v = computeGloveSentence(qlist, emb)\n",
    "print(X_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5942ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 基于BERT的句子向量计算\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "sentence_embedding = np.ones((len(qlist), 768))\n",
    "# 加载Bert模型，model，dataset_name,须指定\n",
    "bert_embedding = BertEmbedding(model='bert_12_768_12', dataset_name='wiki_multilingual_cased')\n",
    "# 查询所有句子的Bert  embedding\n",
    "all_embedding = bert_embedding(qlist, 'sum')\n",
    "for q in qlist:\n",
    "    for i in range(len(all_embedding)):\n",
    "        print(all_embedding[i][1])\n",
    "        # 将每一个句子中每一个词的向量进行拼接求平均得到句子向量\n",
    "        sentence_embedding[i] = np.sum(all_embedding[i][1], axis=0) / len(q.strip().split(' '))\n",
    "        if i == 0:\n",
    "            print(sentence_embedding[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一个句子的向量结果存放在X_bert矩阵里。行数为句子的总个数，列数为一个句子embedding大小。\n",
    "X_bert = sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 第三部分： 相似度匹配以及搜索\n",
    "# 在这部分里，我们需要把用户每一个输入跟知识库里的每一个问题做一个相似度计算，从而得出最相似的问题。但对于这个问题，时间复杂度其实很高，所以我们需要结合倒排表来获取相似度最高的问题，从而获得答案。\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# #### 3.1 tf-idf + 余弦相似度\n",
    "# 我们可以直接基于计算出来的``tf-idf``向量，计算用户最新问题与库中存储的问题之间的相似度，从而选择相似度最高的问题的答案。\n",
    "# 这个方法的复杂度为``O(N)``， ``N``是库中问题的个数。\n",
    "\n",
    "import queue as Q\n",
    "\n",
    "# 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
    "que = Q.PriorityQueue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b773ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(vec1, vec2):\n",
    "    # 定义余弦相似度,余弦相似度越大，两个向量越相似\n",
    "    return np.dot(vec1, vec2.T) / (np.sqrt(np.sum(vec1 ** 2)) * np.sqrt(np.sum(vec2 ** 2)))\n",
    "\n",
    "\n",
    "def get_top_results_tfidf_noindex(query):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
    "    :param query:用户新问题\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 对于用户的输入 query 首先做一系列的预处理(上面提到的方法)，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    2. 计算跟每个库里的问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    top = 5\n",
    "    # 将用户输入的新问题用tf-idf来表示\n",
    "    query_tfidf = computeSentenceEach(query.lower(), vectorizer, word2id)\n",
    "    for i, vec in enumerate(X_tfidf):\n",
    "        # 计算原问题与用户输入的新问题的相似度\n",
    "        result = cosineSimilarity(vec, query_tfidf)\n",
    "        # print(result)\n",
    "        # 存放到大顶堆里面\n",
    "        que.put((-1 * result, i))\n",
    "    i = 0\n",
    "    top_idxs = []\n",
    "    while (i < top and not que.empty()):\n",
    "        # top_idxs存放相似度最高的（存在qlist里的）问题的下标\n",
    "        top_idxs.append(que.get()[1])\n",
    "        i += 1\n",
    "    print(top_idxs)\n",
    "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
    "    return np.array(alist)[top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f269f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给定用户输入的问题 query, 返回最有可能的TOP 5问题的答案。\n",
    "results = get_top_results_tfidf_noindex('In what city and state did Beyonce  grow up')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc = dict()\n",
    "# key:word,value:包含该词的句子序号的列表\n",
    "for i, q in enumerate(qlist):\n",
    "    words = q.strip().split(' ')\n",
    "    for w in set(words):\n",
    "        if w not in word_doc:\n",
    "            # 没在word_doc中的，建立一个空listi\n",
    "            word_doc[w] = set([])\n",
    "        word_doc[w] = word_doc[w] | set([i])\n",
    "\n",
    "# 定一个一个简单的倒排表，是一个map结构。 循环所有qlist一遍就可以\n",
    "inverted_idx = word_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4be8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 3.3 语义相似度\n",
    "# 读取语义相关的单词\n",
    "import codecs\n",
    "\n",
    "\n",
    "def get_related_words(filename):\n",
    "    \"\"\"\n",
    "    从预处理的相似词的文件加载相似词信息\n",
    "    文件格式w1 w2 w3..w11,其中w1为原词，w2-w11为w1的相似词\n",
    "    :param filename: 文件名\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    related_words = {}\n",
    "    with codecs.open(filename, 'r', 'utf8') as Fin:\n",
    "        lines = Fin.readlines()\n",
    "    for line in lines:\n",
    "        words = line.strip().split(' ')\n",
    "        # 键为原词，值为相似词\n",
    "        related_words[words[0]] = words[1:]\n",
    "    return related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18151b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从预处理的相似词的文件加载相似词信息\n",
    "related_words = get_related_words('data/related_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 3.4 利用倒排表搜索\n",
    "# 在这里，我们使用倒排表先获得一批候选问题，然后再通过余弦相似度做精准匹配，这样一来可以节省大量的时间。搜索过程分成两步：\n",
    "# - 使用倒排表把候选问题全部提取出来。首先，对输入的新问题做分词等必要的预处理工作，然后对于句子里的每一个单词，从``related_words``里提取出跟它意思相近的top 10单词， 然后根据这些top词从倒排表里提取相关的文档，把所有的文档返回。 这部分可以放在下面的函数当中，也可以放在外部。\n",
    "# - 然后针对于这些文档做余弦相似度的计算，最后排序并选出最好的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue as Q\n",
    "\n",
    "\n",
    "def cosineSimilarity(vec1, vec2):\n",
    "    # 定义余弦相似度\n",
    "    return np.dot(vec1, vec2.T) / (np.sqrt(np.sum(vec1 ** 2)) * np.sqrt(np.sum(vec2 ** 2)))\n",
    "\n",
    "\n",
    "def getCandidate(query):\n",
    "    \"\"\"\n",
    "    根据查询句子中每个词及其10个相似词所在的序号列表，求交集\n",
    "    :param query: 问题\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    searched = set()\n",
    "    for w in query.strip().split(' '):\n",
    "        # 如果单词不在word2id中或者不在倒排表中\n",
    "        if w not in word2id or w not in inverted_idx:\n",
    "            continue\n",
    "        # 搜索原词所在的序号列表\n",
    "        if len(searched) == 0:\n",
    "            searched = set(inverted_idx[w])\n",
    "        else:\n",
    "            searched = searched & set(inverted_idx[w])\n",
    "        # 搜索相似词所在的列表\n",
    "        if w in related_words:\n",
    "            for similar in related_words[w]:\n",
    "                searched = searched & set(inverted_idx[similar])\n",
    "    return searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6983a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_results_tfidf(query):\n",
    "    \"\"\"\n",
    "    基于TF-IDF,给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
    "    这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    top = 5\n",
    "    # 计算给定句子的TF-IDF\n",
    "    query_tfidf = computeSentenceEach(query, vectorizer, word2id)\n",
    "    # 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
    "    results = Q.PriorityQueue()\n",
    "    # 利用倒排索引表获取相似问题\n",
    "    searched = getCandidate(query)\n",
    "    # print(len(searched))\n",
    "    for candidate in searched:\n",
    "        # 计算candidate与query的余弦相似度\n",
    "        result = cosineSimilarity(query_tfidf, X_tfidf[candidate])\n",
    "        # 优先级队列中保存相似度和对应的candidate序号\n",
    "        # -1保证降序\n",
    "        results.put((-1 * result, candidate))\n",
    "    i = 0\n",
    "    # top_idxs存放相似度最高的（存在qlist里的）问题的索引\n",
    "    top_idxs = []\n",
    "    # hint: 利用priority queue来找出top results.\n",
    "    while i < top and not results.empty():\n",
    "        top_idxs.append(results.get()[1])\n",
    "        i += 1\n",
    "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
    "    return np.array(alist)[top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_results_w2v(query):\n",
    "    \"\"\"\n",
    "    基于word2vec，给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
    "    这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    # embedding用glove\n",
    "    top = 5\n",
    "    # 利用glove将问题进行编码\n",
    "    query_emb = computeGloveSentenceEach(query, emb)\n",
    "    # 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
    "    results = Q.PriorityQueue()\n",
    "    # 利用倒排索引表获取相似问题\n",
    "    searched = getCandidate(query)\n",
    "    for candidate in searched:\n",
    "        # 计算candidate与query的余弦相似度\n",
    "        result = cosineSimilarity(query_emb, X_w2v[candidate])\n",
    "        # 优先级队列中保存相似度和对应的candidate序号\n",
    "        # -1保证降序\n",
    "        results.put((-1 * result, candidate))\n",
    "    # top_idxs存放相似度最高的（存在qlist里的）问题的索引\n",
    "    top_idxs = []\n",
    "    i = 0\n",
    "    # hint: 利用priority queue来找出top results.\n",
    "    while i < top and not results.empty():\n",
    "        top_idxs.append(results.get()[1])\n",
    "        i += 1\n",
    "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
    "    return np.array(alist)[top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c98a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_results_bert(query):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。\n",
    "    这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words).\n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "    top = 5\n",
    "    # embedding用Bert embedding\n",
    "    query_emb = np.sum(bert_embedding([query], 'sum')[0][1], axis=0) / len(query.strip().split())\n",
    "    # 优先级队列实现大顶堆Heap,每次输出都是相似度最大值\n",
    "    results = Q.PriorityQueue()\n",
    "    # 利用倒排索引表获取相似问题\n",
    "    searched = getCandidate(query)\n",
    "    for candidate in searched:\n",
    "        # 计算candidate与query的余弦相似度\n",
    "        result = cosineSimilarity(query_emb, X_bert[candidate])\n",
    "        # 优先级队列中保存相似度和对应的candidate序号\n",
    "        # -1保证降序\n",
    "        results.put((-1 * result, candidate))\n",
    "    # top_idxs存放相似度最高的（存在qlist里的）问题的索引\n",
    "    top_idxs = []\n",
    "    i = 0\n",
    "    # hint: 利用priority queue来找出top results.\n",
    "    while i < top and not results.empty():\n",
    "        top_idxs.append(results.get()[1])\n",
    "        i += 1\n",
    "    # 返回相似度最高的问题对应的答案，作为TOP5答案\n",
    "    return np.array(alist)[top_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed31ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
