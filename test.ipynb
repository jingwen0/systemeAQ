{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingwen0/systemeAQ/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWsbtHcUhWR-",
        "outputId": "49515c42-202a-43e7-9746-361a113f46f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.unicode` is a deprecated alias for `np.compat.unicode`. To silence this warning, use `np.compat.unicode` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `str` for which `np.compat.unicode` is itself an alias. Doing this will not modify any behaviour and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "import torch.optim as optim\n",
        "from numpy import unicode\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from sklearn import model_selection,preprocessing\n",
        "import re\n",
        "import unicodedata\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textblob\n",
        "import string\n",
        "import time\n",
        "import json\n",
        "import pprint\n",
        "from itertools import chain\n",
        "import nltk\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AsSmTFrk5RDB"
      },
      "outputs": [],
      "source": [
        "def seg_sentence(data):\n",
        "    # seg_words = word_tokenize(data, language=\"french\")\n",
        "    seg_words = data.split(\" \")# 空格分词\n",
        "\n",
        "    return seg_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CqgRqlV25mvI"
      },
      "outputs": [],
      "source": [
        "def read_csv(filename):\n",
        "    # 加载数据\n",
        "    df = pd.read_csv(filename, sep=',',error_bad_lines=False)\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "    print(list(df.columns))\n",
        "    text = df['text']\n",
        "    label = df['label']\n",
        "    print(\"总条数：\",len(label),'\\t',len(text))\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zrzYGoNL7PlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd6fa0f-84ae-4f8e-d917-06a55017a688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2022-06-26 10:38:46--  https://raw.githubusercontent.com/weekend1111/sa/main/corpus_annote.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 746317 (729K) [text/plain]\n",
            "Saving to: ‘data/corpus_annote.csv.1’\n",
            "\n",
            "corpus_annote.csv.1 100%[===================>] 728.83K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-06-26 10:38:46 (15.9 MB/s) - ‘data/corpus_annote.csv.1’ saved [746317/746317]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget -P data https://raw.githubusercontent.com/weekend1111/sa/main/corpus_annote.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h-T-bAUcYMfY"
      },
      "outputs": [],
      "source": [
        "def cleaning_text(text):\n",
        "    #Input: list of sentences\n",
        "    #Output: list of sentences\n",
        "    output = [re.sub('  ', ' ', x.lower()) for x in text]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZuSTOat6Qvt",
        "outputId": "35c8ef8d-debb-4378-c88a-e5272b8768e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id                                               text  label\n",
            "0  7101  Mon trésor, mon trésor ! On va se faire bouffe...      1\n",
            "1  6114  Julian, ici toi père. Même si en ce moment, ba...      0\n",
            "2   438  Depuis la prison, il est encore plus méfiant e...     -1\n",
            "3  1904  J'ai fait ma part. Je veux l'argent. Pas ce soir.     -1\n",
            "4  2299  Non. Eh bien, croyez-moi, je sais ce que je fais.      1\n",
            "(9988, 3)\n",
            "['id', 'text', 'label']\n",
            "总条数： 9988 \t 9988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "# 获取原始数据\n",
        "filename = './data/corpus_annote.csv'\n",
        "texts, labels = read_csv(filename)\n",
        "\n",
        "# 数据预处理工作\n",
        "clean_texts = cleaning_text(list(texts))#分词"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pvN_vunzZZeP"
      },
      "outputs": [],
      "source": [
        "def data_process(old_texts,texts, labels):\n",
        "    seg_sentences = []\n",
        "    new_labels = []\n",
        "    sentences = []\n",
        "    for i in range(len(labels)):\n",
        "        data = texts[i]\n",
        "        label = int(labels[i])\n",
        "        old_data = old_texts[i]\n",
        "        if label == 1 or  label == -1 or label == 0:\n",
        "\n",
        "            word_tokens = seg_sentence(data)# 分词\n",
        "            \n",
        "            sentences.append(data)\n",
        "            seg_sentences.append(word_tokens)\n",
        "            new_labels.append(label+1)\n",
        "        else:\n",
        "            continue\n",
        "    return seg_sentences,sentences,new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfkH05BjaKhC",
        "outputId": "59ee25c1-87f9-4a43-c2e3-75af7df37b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 \t mon trésor, mon trésor ! on va se faire bouffer les couilles ! \t ['mon', 'trésor,', 'mon', 'trésor', '!', 'on', 'va', 'se', 'faire', 'bouffer', 'les', 'couilles', '!']\n"
          ]
        }
      ],
      "source": [
        "seg_sentences,sentences,new_labels = data_process(texts, clean_texts, labels)\n",
        "print(new_labels[0],'\\t',sentences[0],'\\t',seg_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdEucEZ06jGu",
        "outputId": "2bde832f-f66e-4d45-c33d-1f0e9f46e5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8989\n",
            "999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# 切分数据\n",
        "ids = [i for i in range(len(new_labels))]\n",
        "train_x_id, valid_x_id, train_y, valid_y = model_selection.train_test_split(ids, new_labels, test_size=0.1, random_state=5) #random_state为空时说明每次随机切分都不一样。测试时要固定测试集，随便设置一个数值。最好的办法还是一开始就把训练集和测试集分别写不同文件，永久固定\n",
        "sentences = np.array(sentences)\n",
        "train_x = sentences[np.array(train_x_id)]# 8989\n",
        "valid_x = sentences[np.array(valid_x_id)]# 999\n",
        "\n",
        "seg_sentences = np.array(seg_sentences)\n",
        "train_seg_x = seg_sentences[np.array(train_x_id)]\n",
        "valid_seg_x = seg_sentences[np.array(valid_x_id)]\n",
        "print(len(train_x))\n",
        "print(len(valid_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "H_Lk8tjP5oge"
      },
      "outputs": [],
      "source": [
        "def encode_samples(tokenized_samples, word_to_idx,unk_id):\n",
        "    features = []\n",
        "    for sample in tokenized_samples:\n",
        "        feature = []\n",
        "        for token in sample:\n",
        "            if token in word_to_idx:\n",
        "                feature.append(word_to_idx[token])\n",
        "            else:\n",
        "                feature.append(unk_id)\n",
        "        features.append(feature)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "c4OimlgY5vYx"
      },
      "outputs": [],
      "source": [
        "def pad_samples(features, maxlen=500, PAD=0):\n",
        "    padded_features = []\n",
        "    for feature in features:\n",
        "        if len(feature) >= maxlen:\n",
        "            padded_feature = feature[:maxlen]\n",
        "        else:\n",
        "            padded_feature = feature\n",
        "            while(len(padded_feature) < maxlen):\n",
        "                padded_feature.append(PAD)\n",
        "        padded_features.append(padded_feature)\n",
        "    return padded_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZpJ5j-l_5xk9"
      },
      "outputs": [],
      "source": [
        "def update_vector(weight, wvmodel, word_to_idx):\n",
        "    # wv_size = len(wvmodel)\n",
        "    # print('wv_size:',wv_size)\n",
        "\n",
        "    # wv_words = wvmodel.key_to_index\n",
        "    wv_words = wvmodel.index2word \n",
        "    \n",
        "    unk_count = 0\n",
        "    \n",
        "    for word,index in word_to_idx.items():\n",
        "\n",
        "        if word in wv_words:\n",
        "        \n",
        "            vector = np.array(wvmodel.word_vec(word))\n",
        "            weight[index, :] = torch.from_numpy(vector)\n",
        "            #print('++++++++',word, index)\n",
        "        else:\n",
        "            #print(\"~~~~UNK:\", word, index) \n",
        "            vector = weight[1]#UNK\n",
        "            weight[index, :] = vector\n",
        "            unk_count+=1\n",
        "    print('unk_count:',unk_count)# UNK1888\n",
        "\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4_fKj-rr55wI"
      },
      "outputs": [],
      "source": [
        "class LSTMNet(nn.Module):\n",
        "    # 初始化，准备工作\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
        "        super(LSTMNet, self).__init__(**kwargs)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_layers = num_layers\n",
        "        self.use_gpu = use_gpu\n",
        "        self.bidirectional = bidirectional\n",
        "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
        "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
        "                               dropout=0)\n",
        "        if self.bidirectional:\n",
        "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
        "        else:\n",
        "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
        "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
        "        outputs = self.decoder(encoding)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mYj1FSre56if"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "device = torch.device(\"cuda:0\")\n",
        "def train(vocab_size, wvmodel, word_to_idx, train_features, train_labels, test_features, test_labels):\n",
        "\n",
        "    # 设置超参数\n",
        "    num_epochs = 20 # 数据跑几次\n",
        "    embed_size = wvmodel.vector_size # 200维\n",
        "    print('embed_size:',embed_size)\n",
        "    num_hiddens = embed_size\n",
        "    num_layers = 2\n",
        "    bidirectional = True\n",
        "    batch_size = 16 # 一次两个句子\n",
        "    labels = 3\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"# Pytorch中使用指定的GPU\n",
        "    #lr = 0.8\n",
        "    lr = 5e-4 # 0.001学习率\n",
        "    gpu_id = 0\n",
        "    \n",
        "\n",
        "    use_gpu = False\n",
        "    \n",
        "    # 使用词向量来初始化词表的表征为，全量词袋\n",
        "    weight = torch.rand(vocab_size+2, embed_size)\n",
        "    # print(weight[2])# 检查是否更新\n",
        "    weight = update_vector(weight, wvmodel, word_to_idx)\n",
        "    #print(weight[2])\n",
        "    print('weight.shape:',weight.shape)\n",
        "    \n",
        "    # 初始化模型，可替换\n",
        "    net = LSTMNet(vocab_size=vocab_size+2, embed_size=embed_size,\n",
        "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
        "                   bidirectional=bidirectional, weight=weight,\n",
        "                   labels=labels, use_gpu=use_gpu)\n",
        "    print(\"device: \", device)            \n",
        "    net.to(device)\n",
        "    \n",
        "    # exit()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    \n",
        "    # 加载数据，并随机打乱训练集\n",
        "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_set = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                        shuffle=False)\n",
        "    print(\"test_iter\", test_iter)\n",
        "    \n",
        "    # 开始训练\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        train_loss, test_losses = 0, 0\n",
        "        train_acc, test_acc = 0, 0\n",
        "        n, m = 0, 0\n",
        "        for feature, label in train_iter:\n",
        "            n += 1\n",
        "            net.train()\n",
        "            net.zero_grad()\n",
        "            feature=feature.to(device)#\n",
        "            label=label.to(device)\n",
        "            score = net(feature)\n",
        "            loss = loss_function(score, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
        "                                                     dim=1), label.cpu())\n",
        "            train_loss += loss\n",
        "            #print(\"train_loss\",train_loss)\n",
        "        with torch.no_grad():\n",
        "            for test_feature, test_label in test_iter:\n",
        "                m += 1\n",
        "                net.eval()\n",
        "                test_feature=test_feature.to(device)\n",
        "                test_label=test_label.to(device)\n",
        "\n",
        "                test_score = net(test_feature)\n",
        "                test_loss = loss_function(test_score, test_label)\n",
        "                test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
        "                                                        dim=1), test_label.cpu())\n",
        "                test_losses += test_loss\n",
        "                #print(\"test_losses\",test_losses)\n",
        "        end = time.time()\n",
        "        runtime = end - start\n",
        "        print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
        "              (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3zRBw7JV__VG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28f3f24-6168-467a-dba2-90f0aa2028b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-26 10:38:47--  https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\n",
            "Resolving embeddings.net (embeddings.net)... 212.107.17.115\n",
            "Connecting to embeddings.net (embeddings.net)|212.107.17.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 126052447 (120M) [application/octet-stream]\n",
            "Saving to: ‘data/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin.1’\n",
            "\n",
            "frWac_non_lem_no_po   8%[>                   ]   9.77M  1.65MB/s    eta 67s    ^C\n"
          ]
        }
      ],
      "source": [
        "!wget -P data https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0qsPwj9bJu74"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors, Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xdlFbKaQJkKy"
      },
      "outputs": [],
      "source": [
        "# 加载词向量\n",
        "model = KeyedVectors.load_word2vec_format(\"data/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\", binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttHsYJ1f6Hxd",
        "outputId": "ac5ae847-57d0-42be-8d0e-0faa646c9310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 17152\n"
          ]
        }
      ],
      "source": [
        "# 获取训练集的词表\n",
        "vocab = set(chain(*train_seg_x))\n",
        "vocab_size = len(vocab)\n",
        "#print(vocab)\n",
        "print('vocab_size:',vocab_size)#训练集词表太小了,测试集UNK会很多"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "f3Qr_vQ664_D"
      },
      "outputs": [],
      "source": [
        "# 构建查表字典 id:word, word:id   pad = 0 unk = 1\n",
        "pad_id = 0\n",
        "unk_id = 1\n",
        "word_to_idx = {word: i+2 for i, word in enumerate(vocab)}\n",
        "word_to_idx['<unk>'] = unk_id\n",
        "idx_to_word = {i+2: word for i, word in enumerate(vocab)}\n",
        "idx_to_word[unk_id] = '<unk>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "p16cH-9zjZ7d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW7E-qNF7GdL",
        "outputId": "cbe50de1-f757-49f0-d35d-b77181ff62c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12137, 11397, 9544, 3296, 8385, 5475, 3624, 8262, 17080, 6824, 8963, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tensor([12137, 11397,  9544,  3296,  8385,  5475,  3624,  8262, 17080,  6824,\n",
            "         8963,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "torch.Size([8989, 20])\n",
            "torch.Size([999, 20])\n"
          ]
        }
      ],
      "source": [
        "# 句子转换成id，并补齐长度\n",
        "max_lenth = 20\n",
        "train_ids = encode_samples(train_seg_x, word_to_idx, unk_id) # unk为1\n",
        "train_features = torch.tensor(pad_samples(train_ids, max_lenth, pad_id))# pad为0\n",
        "train_labels = torch.tensor(train_y)\n",
        "    \n",
        "valid_ids = encode_samples(valid_seg_x, word_to_idx, unk_id) # unk为1\n",
        "valid_features = torch.tensor(pad_samples(valid_ids, max_lenth, pad_id))# pad为0\n",
        "valid_labels = torch.tensor(valid_y)\n",
        "print(train_ids[0])\n",
        "print(train_features[0])\n",
        "print(train_features.shape) #[9000, 50]\n",
        "print(valid_features.shape) #[1000, 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc7Yc2mg7L0p",
        "outputId": "b4a7ac53-1b4a-4dad-c206-7206151345b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed_size: 200\n",
            "unk_count: 9346\n",
            "weight.shape: torch.Size([17154, 200])\n",
            "device:  cuda:0\n",
            "test_iter <torch.utils.data.dataloader.DataLoader object at 0x7f3f4b33dcd0>\n",
            "开始训练。。。\n",
            "epoch: 0, train loss: 1.0883, train acc: 0.41, test loss: 1.0792, test acc: 0.48, time: 6.34\n",
            "开始训练。。。\n",
            "epoch: 1, train loss: 1.0721, train acc: 0.47, test loss: 1.0666, test acc: 0.48, time: 6.12\n",
            "开始训练。。。\n",
            "epoch: 2, train loss: 1.0627, train acc: 0.47, test loss: 1.0592, test acc: 0.48, time: 6.16\n",
            "开始训练。。。\n",
            "epoch: 3, train loss: 1.0573, train acc: 0.47, test loss: 1.0550, test acc: 0.48, time: 6.14\n",
            "开始训练。。。\n",
            "epoch: 4, train loss: 1.0541, train acc: 0.47, test loss: 1.0526, test acc: 0.48, time: 6.15\n",
            "开始训练。。。\n",
            "epoch: 5, train loss: 1.0523, train acc: 0.47, test loss: 1.0512, test acc: 0.48, time: 6.15\n",
            "开始训练。。。\n",
            "epoch: 6, train loss: 1.0513, train acc: 0.47, test loss: 1.0504, test acc: 0.48, time: 6.16\n",
            "开始训练。。。\n",
            "epoch: 7, train loss: 1.0508, train acc: 0.47, test loss: 1.0500, test acc: 0.48, time: 6.12\n",
            "开始训练。。。\n",
            "epoch: 8, train loss: 1.0503, train acc: 0.47, test loss: 1.0497, test acc: 0.48, time: 6.03\n",
            "开始训练。。。\n",
            "epoch: 9, train loss: 1.0502, train acc: 0.47, test loss: 1.0496, test acc: 0.48, time: 6.11\n",
            "开始训练。。。\n",
            "epoch: 10, train loss: 1.0500, train acc: 0.47, test loss: 1.0495, test acc: 0.48, time: 6.08\n",
            "开始训练。。。\n",
            "epoch: 11, train loss: 1.0499, train acc: 0.47, test loss: 1.0494, test acc: 0.48, time: 6.95\n",
            "开始训练。。。\n",
            "epoch: 12, train loss: 1.0497, train acc: 0.47, test loss: 1.0493, test acc: 0.48, time: 6.08\n",
            "开始训练。。。\n",
            "epoch: 13, train loss: 1.0497, train acc: 0.47, test loss: 1.0492, test acc: 0.48, time: 6.08\n",
            "开始训练。。。\n",
            "epoch: 14, train loss: 1.0496, train acc: 0.47, test loss: 1.0492, test acc: 0.48, time: 6.12\n",
            "开始训练。。。\n",
            "epoch: 15, train loss: 1.0495, train acc: 0.47, test loss: 1.0492, test acc: 0.48, time: 6.08\n",
            "开始训练。。。\n",
            "epoch: 16, train loss: 1.0494, train acc: 0.47, test loss: 1.0491, test acc: 0.48, time: 6.11\n",
            "开始训练。。。\n",
            "epoch: 17, train loss: 1.0493, train acc: 0.47, test loss: 1.0491, test acc: 0.48, time: 6.56\n",
            "开始训练。。。\n",
            "epoch: 18, train loss: 1.0493, train acc: 0.47, test loss: 1.0490, test acc: 0.48, time: 6.16\n",
            "开始训练。。。\n",
            "epoch: 19, train loss: 1.0492, train acc: 0.47, test loss: 1.0489, test acc: 0.48, time: 6.13\n"
          ]
        }
      ],
      "source": [
        "# 训练\n",
        "train(vocab_size, model, word_to_idx, train_features, train_labels, valid_features, valid_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('py39': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "6e16769360e1a371eb88e122bb40baccd33888c4d502bf84e6dd001be055341b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}