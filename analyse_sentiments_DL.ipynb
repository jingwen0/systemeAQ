{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analyse_sentiments_DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxaWUsB/AniV72GGDyCSPc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingwen0/systemeAQ/blob/main/analyse_sentiments_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWsbtHcUhWR-",
        "outputId": "f757ca3c-1eff-4cd2-ca89-84c4fa2e48d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.unicode` is a deprecated alias for `np.compat.unicode`. To silence this warning, use `np.compat.unicode` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `str` for which `np.compat.unicode` is itself an alias. Doing this will not modify any behaviour and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data.dataloader as dataloader\n",
        "import torch.optim as optim\n",
        "from numpy import unicode\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from sklearn import model_selection,preprocessing\n",
        "import re\n",
        "import unicodedata\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textblob\n",
        "import string\n",
        "import time\n",
        "import json\n",
        "import pprint\n",
        "from itertools import chain\n",
        "import nltk\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seg_sentence(data):\n",
        "    # seg_words = word_tokenize(data, language=\"french\")\n",
        "    seg_words = data.split(\" \")# 空格分词\n",
        "\n",
        "    return seg_words"
      ],
      "metadata": {
        "id": "AsSmTFrk5RDB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv(filename):\n",
        "    # 加载数据\n",
        "    df = pd.read_csv(filename, sep=',',error_bad_lines=False)\n",
        "    print(df.head())\n",
        "    print(df.shape)\n",
        "    print(list(df.columns))\n",
        "    text = df['text']\n",
        "    label = df['label']\n",
        "    print(\"总条数：\",len(label),'\\t',len(text))\n",
        "    return text, label"
      ],
      "metadata": {
        "id": "CqgRqlV25mvI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget -P data https://raw.githubusercontent.com/jingwen0/systemeAQ/main/corpus_mai.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrzYGoNL7PlC",
        "outputId": "7523adf9-994b-42e3-9880-63e168d34b3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-07 20:33:14--  https://raw.githubusercontent.com/jingwen0/systemeAQ/main/corpus_mai.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 746317 (729K) [text/plain]\n",
            "Saving to: ‘data/corpus_mai.csv’\n",
            "\n",
            "corpus_mai.csv      100%[===================>] 728.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-07 20:33:15 (23.1 MB/s) - ‘data/corpus_mai.csv’ saved [746317/746317]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning_text(text):\n",
        "    #Input: list of sentences\n",
        "    #Output: list of sentences\n",
        "    output = [re.sub('  ', ' ', x.lower()) for x in text]\n",
        "    return output"
      ],
      "metadata": {
        "id": "h-T-bAUcYMfY"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取原始数据\n",
        "filename = './data/corpus_mai.csv'\n",
        "texts, labels = read_csv(filename)\n",
        "\n",
        "# 数据预处理工作\n",
        "clean_texts = cleaning_text(list(texts))#分词"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZuSTOat6Qvt",
        "outputId": "4f012bb0-f897-405c-ec31-2a065da09bf3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id                                                               text  \\\n",
            "0  7101     Mon trésor, mon trésor ! On va se faire bouffer les couilles !   \n",
            "1  6114  Julian, ici toi père. Même si en ce moment, basé sur les faits...   \n",
            "2   438          Depuis la prison, il est encore plus méfiant et farouche.   \n",
            "3  1904                  J'ai fait ma part. Je veux l'argent. Pas ce soir.   \n",
            "4  2299                  Non. Eh bien, croyez-moi, je sais ce que je fais.   \n",
            "\n",
            "   label  \n",
            "0      1  \n",
            "1      0  \n",
            "2     -1  \n",
            "3     -1  \n",
            "4      1  \n",
            "(9988, 3)\n",
            "['id', 'text', 'label']\n",
            "总条数： 9988 \t 9988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(old_texts,texts, labels):\n",
        "    seg_sentences = []\n",
        "    new_labels = []\n",
        "    sentences = []\n",
        "    for i in range(len(labels)):\n",
        "        data = texts[i]\n",
        "        label = int(labels[i])\n",
        "        old_data = old_texts[i]\n",
        "        if label == 1 or  label == -1 or label == 0:\n",
        "\n",
        "            word_tokens = seg_sentence(data)# 分词\n",
        "            \n",
        "            sentences.append(data)\n",
        "            seg_sentences.append(word_tokens)\n",
        "            new_labels.append(label)\n",
        "        else:\n",
        "            continue\n",
        "    return seg_sentences,sentences,new_labels"
      ],
      "metadata": {
        "id": "pvN_vunzZZeP"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seg_sentences,sentences,new_labels = data_process(texts, clean_texts, labels)\n",
        "print(new_labels[0],'\\t',sentences[0],'\\t',seg_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfkH05BjaKhC",
        "outputId": "056d935e-3b52-463d-de8a-ab965d2c49c6"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \t mon trésor, mon trésor ! on va se faire bouffer les couilles ! \t ['mon', 'trésor,', 'mon', 'trésor', '!', 'on', 'va', 'se', 'faire', 'bouffer', 'les', 'couilles', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 切分数据\n",
        "ids = [i for i in range(len(new_labels))]\n",
        "train_x_id, valid_x_id, train_y, valid_y = model_selection.train_test_split(ids, new_labels, test_size=0.1, random_state=5) #random_state为空时说明每次随机切分都不一样。测试时要固定测试集，随便设置一个数值。最好的办法还是一开始就把训练集和测试集分别写不同文件，永久固定\n",
        "sentences = np.array(sentences)\n",
        "train_x = sentences[np.array(train_x_id)]\n",
        "valid_x = sentences[np.array(valid_x_id)]\n",
        "\n",
        "seg_sentences = np.array(seg_sentences)\n",
        "train_seg_x = seg_sentences[np.array(train_x_id)]\n",
        "valid_seg_x = seg_sentences[np.array(valid_x_id)]\n",
        "print(len(train_x))\n",
        "print(len(valid_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdEucEZ06jGu",
        "outputId": "fc82278d-4659-4326-c89f-e5a3116be425"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8989\n",
            "999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_samples(tokenized_samples, word_to_idx,unk_id):\n",
        "    features = []\n",
        "    for sample in tokenized_samples:\n",
        "        feature = []\n",
        "        for token in sample:\n",
        "            if token in word_to_idx:\n",
        "                feature.append(word_to_idx[token])\n",
        "            else:\n",
        "                feature.append(unk_id)\n",
        "        features.append(feature)\n",
        "    return features"
      ],
      "metadata": {
        "id": "H_Lk8tjP5oge"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_samples(features, maxlen=500, PAD=0):\n",
        "    padded_features = []\n",
        "    for feature in features:\n",
        "        if len(feature) >= maxlen:\n",
        "            padded_feature = feature[:maxlen]\n",
        "        else:\n",
        "            padded_feature = feature\n",
        "            while(len(padded_feature) < maxlen):\n",
        "                padded_feature.append(PAD)\n",
        "        padded_features.append(padded_feature)\n",
        "    return padded_features"
      ],
      "metadata": {
        "id": "c4OimlgY5vYx"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_vector(weight, wvmodel, word_to_idx):\n",
        "    # wv_size = len(wvmodel)#wv词表:155562\n",
        "    # print('wv_size:',wv_size)\n",
        "\n",
        "    # wv_words = wvmodel.key_to_index\n",
        "    wv_words = wvmodel.index2word\n",
        "    \n",
        "    unk_count = 0\n",
        "    \n",
        "    for word,index in word_to_idx.items():\n",
        "\n",
        "        if word in wv_words:\n",
        "        \n",
        "            vector = np.array(wvmodel.word_vec(word))\n",
        "            weight[index, :] = torch.from_numpy(vector)\n",
        "            #print('++++++++',word, index)\n",
        "        else:\n",
        "            #print(\"~~~~UNK:\", word, index) \n",
        "            vector = weight[1]#UNK\n",
        "            weight[index, :] = vector\n",
        "            unk_count+=1\n",
        "    print('unk_count:',unk_count)# UNK1888\n",
        "\n",
        "    return weight"
      ],
      "metadata": {
        "id": "ZpJ5j-l_5xk9"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNet(nn.Module):\n",
        "    # 初始化，准备工作\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
        "        super(LSTMNet, self).__init__(**kwargs)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_layers = num_layers\n",
        "        self.use_gpu = use_gpu\n",
        "        self.bidirectional = bidirectional\n",
        "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
        "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
        "                               dropout=0)\n",
        "        if self.bidirectional:\n",
        "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
        "        else:\n",
        "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
        "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
        "        outputs = self.decoder(encoding)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4_fKj-rr55wI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def train(vocab_size, wvmodel, word_to_idx, train_features, train_labels, test_features, test_labels):\n",
        "\n",
        "    # 设置超参数\n",
        "    num_epochs = 20 # 数据跑几次\n",
        "    embed_size = wvmodel.vector_size # 300维\n",
        "    print('embed_size:',embed_size)\n",
        "    num_hiddens = embed_size\n",
        "    num_layers = 2\n",
        "    bidirectional = True\n",
        "    batch_size = 2 # 一次两个句子\n",
        "    labels = 3\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "    #lr = 0.8\n",
        "    lr = 1e-3 # 0.001学习率\n",
        "    device = torch.device(\"cpu:0\")\n",
        "    gpu_id = -1\n",
        "    device = torch.device('cuda:{}'.format(gpu_id) if torch.cuda.is_available() and gpu_id>=0 else torch.device('cpu'))\n",
        "\n",
        "    use_gpu = False\n",
        "    \n",
        "    # 使用词向量来初始化词表的表征为，10687=全量词袋\n",
        "    weight = torch.rand(vocab_size+2, embed_size)# [10687+2, 200]\n",
        "    # print(weight[2])# 检查是否更新\n",
        "    weight = update_vector(weight, wvmodel, word_to_idx)\n",
        "    #print(weight[2])\n",
        "    print('weight.shape:',weight.shape)\n",
        "    \n",
        "    # 初始化模型，可替换\n",
        "    net = LSTMNet(vocab_size=(vocab_size+2), embed_size=embed_size,\n",
        "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
        "                   bidirectional=bidirectional, weight=weight,\n",
        "                   labels=labels, use_gpu=use_gpu)\n",
        "    device = net.to(device)\n",
        "    print(device)\n",
        "    exit()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    \n",
        "    # 加载数据，并随机打乱训练集\n",
        "    train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
        "    test_set = torch.utils.data.TensorDataset(test_features, test_labels)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "    test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                        shuffle=False)\n",
        "    \n",
        "    # 开始训练\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        train_loss, test_losses = 0, 0\n",
        "        train_acc, test_acc = 0, 0\n",
        "        n, m = 0, 0\n",
        "        for feature, label in train_iter:\n",
        "            n += 1\n",
        "            #net.train()\n",
        "            net.zero_grad()\n",
        "            feature = Variable(feature.cuda())\n",
        "            label = Variable(label.cuda())\n",
        "            score = net(feature)\n",
        "            loss = loss_function(score, label)\n",
        "            loss.backward()\n",
        "            #scheduler.step()\n",
        "            optimizer.step()\n",
        "            train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
        "                                                     dim=1), label.cpu())\n",
        "            train_loss += loss\n",
        "        with torch.no_grad():\n",
        "            for test_feature, test_label in test_iter:\n",
        "                m += 1\n",
        "                #net.eval()\n",
        "                test_feature = test_feature.cuda()\n",
        "                test_label = test_label.cuda()\n",
        "                test_score = net(test_feature)\n",
        "                test_loss = loss_function(test_score, test_label)\n",
        "                test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
        "                                                        dim=1), test_label.cpu())\n",
        "                test_losses += test_loss\n",
        "        end = time.time()\n",
        "        runtime = end - start\n",
        "        print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
        "              (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n"
      ],
      "metadata": {
        "id": "mYj1FSre56if"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TesibJz34MUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P data https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/model_26.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zRBw7JV__VG",
        "outputId": "162b8c59-d28d-41c8-bbfc-8c0f1498f89c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-07 22:25:03--  https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/model_26.txt\n",
            "Resolving git.unistra.fr (git.unistra.fr)... 130.79.254.48\n",
            "Connecting to git.unistra.fr (git.unistra.fr)|130.79.254.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17389513 (17M) [text/plain]\n",
            "Saving to: ‘data/model_26.txt’\n",
            "\n",
            "model_26.txt        100%[===================>]  16.58M  8.87MB/s    in 1.9s    \n",
            "\n",
            "2022-06-07 22:25:06 (8.87 MB/s) - ‘data/model_26.txt’ saved [17389513/17389513]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open('data/model_26.txt', 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      word, coefs = line.split(maxsplit=1)\n",
        "      coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "print(f'{len(embeddings_index)} vecteurs de mots ont été lus')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lAFV5SNRD7p",
        "outputId": "5ad28e90-7acb-45c2-e690-3544d5b022ba"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6082 vecteurs de mots ont été lus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取训练集的词表\n",
        "vocab = set(chain(*train_seg_x))\n",
        "vocab_size = len(vocab)\n",
        "#print(vocab)\n",
        "print('vocab_size:',vocab_size)#10581,训练集词表太小了,测试集UNK会很多\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttHsYJ1f6Hxd",
        "outputId": "36920eac-28b6-4c65-96ac-6eb176004572"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 17152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建查表字典 id:word, word:id   pad = 0 unk = 1\n",
        "pad_id = 0\n",
        "unk_id = 1\n",
        "word_to_idx = {word: i+2 for i, word in enumerate(vocab)}\n",
        "word_to_idx['<unk>'] = unk_id\n",
        "idx_to_word = {i+2: word for i, word in enumerate(vocab)}\n",
        "idx_to_word[unk_id] = '<unk>'"
      ],
      "metadata": {
        "id": "f3Qr_vQ664_D"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(vocab)\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Préparation de la matrice\n",
        "# Les mots qui ne se trouvent pas dans les plongements pré-entraînés seront \n",
        "# représentés par des vecteurs dont toutes les composantes sont égales à 0,\n",
        "# y compris la représentation utilisée pour compléter les documents courts et\n",
        "# celle utilisée pour les mots inconnus [UNK]\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_to_idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(f'{hits} mots ont été trouvés dans les plongements pré-entraînés')\n",
        "print(f'{misses} sont absents')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX-mHq0f6_Sk",
        "outputId": "b68f1805-5350-4bb8-99c4-52407cda987f"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "547 mots ont été trouvés dans les plongements pré-entraînés\n",
            "16606 sont absents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 句子转换成id，并补齐长度\n",
        "max_lenth = 20\n",
        "train_ids = encode_samples(train_seg_x, word_to_idx, unk_id) # unk为1\n",
        "train_features = torch.tensor(pad_samples(train_ids, max_lenth, pad_id))# pad为0\n",
        "train_labels = torch.tensor(train_y)\n",
        "    \n",
        "valid_ids = encode_samples(valid_seg_x, word_to_idx, unk_id) # unk为1\n",
        "valid_features = torch.tensor(pad_samples(valid_ids, max_lenth, pad_id))# pad为0\n",
        "valid_labels = torch.tensor(valid_y)\n",
        "print(train_ids[0])\n",
        "print(train_features[0])\n",
        "print(train_features.shape) #[9000, 50]\n",
        "print(valid_features.shape) #[1000, 50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW7E-qNF7GdL",
        "outputId": "3d588574-47f7-4dcc-9ea1-af1e54ba107c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4201, 7324, 5819, 9529, 1687, 6413, 6845, 7313, 6617, 9047, 8525, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "tensor([4201, 7324, 5819, 9529, 1687, 6413, 6845, 7313, 6617, 9047, 8525,    2,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "torch.Size([8989, 20])\n",
            "torch.Size([999, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k5KKUEpBkJVb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 训练\n",
        "train(vocab_size, word2vec_model, word_to_idx, train_features, train_labels, valid_features, valid_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "hc7Yc2mg7L0p",
        "outputId": "21244eb2-09ff-48ba-9290-e71097e8e3ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed_size: 300\n",
            "unk_count: 3236\n",
            "weight.shape: torch.Size([10583, 300])\n",
            "LSTMNet(\n",
            "  (embedding): Embedding(10583, 300)\n",
            "  (encoder): LSTM(300, 300, num_layers=2, bidirectional=True)\n",
            "  (decoder): Linear(in_features=1200, out_features=3, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ceb4e9c80582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-eb7bf72077fd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(vocab_size, wvmodel, word_to_idx, train_features, train_labels, test_features, test_labels)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-33ce2a3ceb76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(vocab)\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "EMBEDDING_DIM = 32\n",
        "MAX_LENGTH = 20\n",
        "PADDING_TYPE = \"post\"\n",
        "TRUNC_TYPE = \"post\"\n",
        "OUTPUT_SHAPE = len(set(train_labels))"
      ],
      "metadata": {
        "id": "xiMvm7fZ4176"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")"
      ],
      "metadata": {
        "id": "rXjCx-H07kiv"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listes utilisées pour sauvegarder les résultats obtenus à chaque pli\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "histories = []\n",
        "folds = 50\n",
        "stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, \n",
        "                                             random_state=12)\n",
        "fold_no = 1\n",
        "for train, test in stratkfold.split(train_x, train_y):\n",
        "  # Création du modèle\n",
        "  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  embedding_layer = Embedding(num_tokens, embedding_dim, trainable=True,\n",
        "      embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "  )\n",
        "  embedded_sequences = embedding_layer(int_sequences_input)\n",
        "  x = layers.Conv1D(64, 5, activation=\"relu\")(embedded_sequences)\n",
        "  x = layers.MaxPooling1D(5)(x)\n",
        "  x = layers.Conv1D(64, 5, activation=\"relu\")(x)\n",
        "  x = layers.GlobalMaxPooling1D()(x)\n",
        "  x = layers.Dense(64, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "  model = keras.Model(int_sequences_input, preds)\n",
        "  model.summary()\n",
        "  \n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Entraînement pour le pli {fold_no} ...')\n",
        "  fold_x_train = vectorizer(X_train.iloc[train].to_numpy()).numpy()\n",
        "  fold_x_val = vectorizer(X_train.iloc[test].to_numpy()).numpy()\n",
        "  fold_y_train = y_train.iloc[train].to_numpy()\n",
        "  fold_y_val = y_train.iloc[test].to_numpy()\n",
        "\n",
        "  # Compilation du modèle : permet de préciser la fonction de perte et l'optimiseur\n",
        "  # loss=sparse_categorical_crossentropy : entropie croisée, dans le cas où les \n",
        "  #  classes cibles sont indiquées sous forme d'entiers. Il s'agira de minimiser\n",
        "  #  la perte pendant l'apprentissage\n",
        "  # optimizer=rmsprop : l'optimiseur détermine la manière doit les poids seront\n",
        "  #  mis à jour pendant l'apprentissage\n",
        "  model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        "  )\n",
        "  # Entraînement\n",
        "  history = model.fit(fold_x_train, fold_y_train, batch_size=128, epochs=10, \n",
        "            validation_data=(fold_x_val, fold_y_val))\n",
        "  histories.append(history)\n",
        "  # Evaluation sur les données de validation\n",
        "  scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n",
        "  print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n",
        "        f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# Affichage des scores moyens par pli\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Scores par pli')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('---------------------------------------------------------------------')\n",
        "  print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n",
        "        f'- Accuracy: {acc_per_fold[i]:.2f}%')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Scores moyens pour tous les plis :')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n",
        "      f'(+- {np.std(acc_per_fold):.2f})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
        "print('---------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "NDzwJerueFjD",
        "outputId": "557d7ae1-0654-498a-9ba0-6d8e17d886f4"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-b60a12ad37c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   )\n\u001b[1;32m     15\u001b[0m   \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_sequences_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = Sequential([\n",
        "        embedded_sequences,\n",
        "        Dropout(0.2),\n",
        "        Conv1D(filters = 256, kernel_size = 3, activation = \"relu\"),\n",
        "        MaxPooling1D(pool_size = 3),\n",
        "        Conv1D(filters = 128, kernel_size = 3, activation = \"relu\"),\n",
        "        MaxPooling1D(pool_size = 3),\n",
        "        LSTM(128),\n",
        "        Dense(128, activation = \"relu\"),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation = \"relu\"),\n",
        "        Dense(300, activation = \"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "U-nyEG-V4z_l",
        "outputId": "77ee48e0-b2e5-451a-fa6e-287321f37cce"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-8c0612f7acce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m ])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       raise TypeError('The added layer must be an instance of class Layer. '\n\u001b[0m\u001b[1;32m    179\u001b[0m                       f'Received: layer={layer} of type {type(layer)}.')\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Received: layer=KerasTensor(type_spec=TensorSpec(shape=(None, None, 300), dtype=tf.float32, name=None), name='embedding_4/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_4'\") of type <class 'keras.engine.keras_tensor.KerasTensor'>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ta7hZoJGwrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f-VUn6bfGt20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGkNCgbj6JD_",
        "outputId": "1b8f491a-9fea-409b-bb63-a9f3d5d661af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 32)            338592    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 20, 32)            0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 18, 256)           24832     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 6, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 4, 128)            98432     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 1, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8989)              584285    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,202,493\n",
            "Trainable params: 1,202,493\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model_rnn.compile(loss = \"sparse_categorical_crossentropy\",\n",
        "              optimizer = OPTIMIZER,\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "roH1PQe88T00"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_tf = tf.constant(train_features,dtype = tf.float32)\n",
        "train_labels_tf = tf .constant(train_labels,dtype = tf.float32)\n",
        "valid_features_tf = tf.constant(valid_features,dtype = tf.float32)\n",
        "valid_labels_tf = tf .constant(valid_labels,dtype = tf.float32)"
      ],
      "metadata": {
        "id": "MB7dmLkPAxcT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_rnn.fit(train_features_tf, train_labels_tf,\n",
        "                    epochs = 150,\n",
        "                    validation_data = (valid_features_tf, valid_labels_tf),\n",
        "                    batch_size = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zDNUPdwD8hjp",
        "outputId": "c20e81e0-e8e6-4a4c-9f82-77147aed8ab8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "141/141 [==============================] - 17s 13ms/step - loss: nan - accuracy: 0.3024 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 2/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 3/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 4/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 5/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 6/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 7/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 8/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 9/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 10/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 11/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 12/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 13/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 14/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 15/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 16/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 17/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 18/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 19/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 20/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 21/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 22/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 23/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 24/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 25/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 26/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 27/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 28/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 29/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 30/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 31/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 32/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 33/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 34/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 35/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 36/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 37/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 38/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 39/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 40/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 41/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 42/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 43/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 44/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 45/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 46/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 47/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 48/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 49/150\n",
            "141/141 [==============================] - 2s 13ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 50/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 51/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 52/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 53/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 54/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 55/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 56/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 57/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 58/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 59/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 60/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 61/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 62/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 63/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 64/150\n",
            "141/141 [==============================] - 2s 11ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 65/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 66/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 67/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 68/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 69/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 70/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 71/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 72/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 73/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 74/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 75/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 76/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 77/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 78/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 79/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 80/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 81/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 82/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 83/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 84/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 85/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 86/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 87/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 88/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 89/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 90/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 91/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 92/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 93/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 94/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 95/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 96/150\n",
            "141/141 [==============================] - 2s 13ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 97/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 98/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 99/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 100/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 101/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 102/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 103/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 104/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 105/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 106/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 107/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 108/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 109/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 110/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 111/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 112/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 113/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 114/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 115/150\n",
            "141/141 [==============================] - 1s 10ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 116/150\n",
            "141/141 [==============================] - 3s 19ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 117/150\n",
            "141/141 [==============================] - 2s 17ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 118/150\n",
            "141/141 [==============================] - 2s 17ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 119/150\n",
            "141/141 [==============================] - 2s 15ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 120/150\n",
            "141/141 [==============================] - 2s 16ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 121/150\n",
            "141/141 [==============================] - 2s 15ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 122/150\n",
            "141/141 [==============================] - 2s 15ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 123/150\n",
            "141/141 [==============================] - 2s 13ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 124/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 125/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 126/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 127/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 128/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 129/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 130/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 131/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 132/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 133/150\n",
            "141/141 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 134/150\n",
            "141/141 [==============================] - 1s 8ms/step - loss: nan - accuracy: 0.3046 - val_loss: nan - val_accuracy: 0.2863\n",
            "Epoch 135/150\n",
            " 64/141 [============>.................] - ETA: 0s - loss: nan - accuracy: 0.2905"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c3620e688dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_features_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     batch_size = 64)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights = [embedding_matrix], input_length=maxlen))\n",
        "model_cnn.add(layers.Dropout(0.3)) \n",
        "model_cnn.add(layers.Conv1D(filters=32, kernel_size=3, activation=act)) \n",
        "model_cnn.add(layers.MaxPool1D(pool_size=3)) \n",
        "model_cnn.add(layers.Dropout(0.3))\n",
        "model_cnn.add(layers.Conv1D(filters=32, kernel_size=3, activation=act)) \n",
        "model_cnn.add(layers.MaxPool1D(pool_size=3)) \n",
        "model_cnn.add(layers.Dropout(0.3))\n",
        "model_cnn.add(layers.Bidirectional(layers.LSTM(256, recurrent_dropout=0.3)))\n",
        "model_cnn.add(layers.Dropout(0.3))\n",
        "model_cnn.add(layers.Dense(256,activation=act)) \n",
        "model_cnn.add(layers.Dropout(0.3)) \n",
        "model_cnn.add(layers.Dense(6, activation=\"softmax\"))\n",
        "model_cnn.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "model_cnn.summary()"
      ],
      "metadata": {
        "id": "LxYZ8p3m6iMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}